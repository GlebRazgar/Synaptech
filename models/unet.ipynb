{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "from torch.nn.functional import relu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EEGtoMEGUNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Encoder \n",
    "        # First Level\n",
    "        self.e11 = nn.Conv1d(74, 128, kernel_size=3, padding='same')    \n",
    "        self.e12 = nn.Conv1d(128, 128, kernel_size=3, padding='same')   \n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=2, stride=2)         \n",
    "        # [74, 250] → [128, 250] → [128, 125]\n",
    "\n",
    "        # Second Level\n",
    "        self.e21 = nn.Conv1d(128, 256, kernel_size=3, padding='same')   \n",
    "        self.e22 = nn.Conv1d(256, 256, kernel_size=3, padding='same')   \n",
    "        self.pool2 = nn.MaxPool1d(kernel_size=2, stride=2) \n",
    "        # [128, 125] → [256, 125] → [256, 62]\n",
    "\n",
    "        # Third Level\n",
    "        self.e31 = nn.Conv1d(256, 512, kernel_size=3, padding='same')   \n",
    "        self.e32 = nn.Conv1d(512, 512, kernel_size=3, padding='same')   \n",
    "        self.pool3 = nn.MaxPool1d(kernel_size=2, stride=2)        \n",
    "        # [256, 62] → [512, 62] → [512, 31]\n",
    "\n",
    "        # Bridge\n",
    "        self.bridge1 = nn.Conv1d(512, 1024, kernel_size=3, padding='same')\n",
    "        self.bridge2 = nn.Conv1d(1024, 1024, kernel_size=3, padding='same')\n",
    "\n",
    "        # Decoder (modified)\n",
    "        self.upconv1 = nn.ConvTranspose1d(1024, 512, kernel_size=2, stride=2, padding=0)\n",
    "        self.d11 = nn.Conv1d(1024, 512, kernel_size=3, padding='same')   \n",
    "        self.d12 = nn.Conv1d(512, 512, kernel_size=3, padding='same')    \n",
    "\n",
    "        self.upconv2 = nn.ConvTranspose1d(512, 256, kernel_size=2, stride=2, padding=0)\n",
    "        self.d21 = nn.Conv1d(512, 256, kernel_size=3, padding='same')    \n",
    "        self.d22 = nn.Conv1d(256, 256, kernel_size=3, padding='same')    \n",
    "\n",
    "        self.upconv3 = nn.ConvTranspose1d(256, 128, kernel_size=2, stride=2, padding=0)\n",
    "        self.d31 = nn.Conv1d(256, 128, kernel_size=3, padding='same')    \n",
    "        self.d32 = nn.Conv1d(128, 128, kernel_size=3, padding='same')    \n",
    "\n",
    "        # Output layer (same as before)\n",
    "        self.outconv = nn.Conv1d(128, 104, kernel_size=1)           \n",
    "        \n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Save input size for later\n",
    "        input_size = x.size(-1)\n",
    "        \n",
    "        # Encoder\n",
    "        xe11 = self.relu(self.e11(x))\n",
    "        xe12 = self.relu(self.e12(xe11))\n",
    "        xp1 = self.pool1(xe12)\n",
    "        xp1 = self.dropout(xp1)\n",
    "\n",
    "        xe21 = self.relu(self.e21(xp1))\n",
    "        xe22 = self.relu(self.e22(xe21))\n",
    "        xp2 = self.pool2(xe22)\n",
    "        xp2 = self.dropout(xp2)\n",
    "\n",
    "        xe31 = self.relu(self.e31(xp2))\n",
    "        xe32 = self.relu(self.e32(xe31))\n",
    "        xp3 = self.pool3(xe32)\n",
    "        xp3 = self.dropout(xp3)\n",
    "\n",
    "        # Bridge\n",
    "        xb1 = self.relu(self.bridge1(xp3))\n",
    "        xb2 = self.relu(self.bridge2(xb1))\n",
    "        xb2 = self.dropout(xb2)\n",
    "\n",
    "        # Decoder with size matching\n",
    "        xu1 = self.upconv1(xb2)\n",
    "        xu1 = torch.nn.functional.pad(xu1, [0, xe32.size(-1) - xu1.size(-1)])  # Pad if necessary\n",
    "        xu11 = torch.cat([xu1, xe32], dim=1)\n",
    "        xd11 = self.relu(self.d11(xu11))\n",
    "        xd12 = self.relu(self.d12(xd11))\n",
    "        xd12 = self.dropout(xd12)\n",
    "\n",
    "        xu2 = self.upconv2(xd12)\n",
    "        xu2 = torch.nn.functional.pad(xu2, [0, xe22.size(-1) - xu2.size(-1)])  # Pad if necessary\n",
    "        xu22 = torch.cat([xu2, xe22], dim=1)\n",
    "        xd21 = self.relu(self.d21(xu22))\n",
    "        xd22 = self.relu(self.d22(xd21))\n",
    "        xd22 = self.dropout(xd22)\n",
    "\n",
    "        xu3 = self.upconv3(xd22)\n",
    "        xu3 = torch.nn.functional.pad(xu3, [0, xe12.size(-1) - xu3.size(-1)])  # Pad if necessary\n",
    "        xu33 = torch.cat([xu3, xe12], dim=1)\n",
    "        xd31 = self.relu(self.d31(xu33))\n",
    "        xd32 = self.relu(self.d32(xd31))\n",
    "\n",
    "        # Output layer\n",
    "        out = self.outconv(xd32)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random array test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 74, 250])\n",
      "Output shape: torch.Size([2, 104, 250])\n",
      "\n",
      "Input data slice (first sample, first 3 channels, first 5 timepoints):\n",
      "tensor([[ 1.2187, -0.1699,  0.5493, -2.3517,  1.1940],\n",
      "        [ 2.2028,  1.4495,  0.7768, -0.9669,  1.4631],\n",
      "        [ 0.9182,  1.2699, -0.2945,  0.3334, -1.8648]])\n",
      "\n",
      "Output data slice (first sample, first 3 channels, first 5 timepoints):\n",
      "tensor([[ 0.0795,  0.0686,  0.0683,  0.0698,  0.0789],\n",
      "        [ 0.0633,  0.0617,  0.0728,  0.0578,  0.0478],\n",
      "        [-0.0118, -0.0090,  0.0022, -0.0050, -0.0039]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Create a test batch of 2 samples\n",
    "batch_size = 2\n",
    "eeg_channels = 74\n",
    "time_points = 250\n",
    "\n",
    "# Create random test data\n",
    "test_data = torch.randn(batch_size, eeg_channels, time_points)\n",
    "print(f\"Input shape: {test_data.shape}\")\n",
    "\n",
    "# Initialize the model\n",
    "model = EEGtoMEGUNet()\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Forward pass\n",
    "with torch.no_grad():\n",
    "    output = model(test_data)\n",
    "    print(f\"Output shape: {output.shape}\")\n",
    "\n",
    "# Print a small slice of input and output to compare\n",
    "print(\"\\nInput data slice (first sample, first 3 channels, first 5 timepoints):\")\n",
    "print(test_data[0, :3, :5])\n",
    "print(\"\\nOutput data slice (first sample, first 3 channels, first 5 timepoints):\")\n",
    "print(output[0, :3, :5])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "synaptech_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
