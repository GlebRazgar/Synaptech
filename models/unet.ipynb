{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "from torch.nn.functional import relu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EEGtoMEGUNet2D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Reshape input to [batch_size, 1, 74, 250] (adding channel dimension)\n",
    "        \n",
    "        # Encoder \n",
    "        # First Level\n",
    "        self.e11 = nn.Conv2d(1, 128, kernel_size=(77, 20), padding=(0, 1), stride=(1, 10))    \n",
    "        self.e12 = nn.Conv2d(128, 128, kernel_size=(1, 3), padding=(0, 1))   \n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=(1, 2), stride=(1, 2))         \n",
    "        # [1, 250] → [128, 250] → [128, 125]\n",
    "\n",
    "        # Second Level\n",
    "        self.e21 = nn.Conv2d(128, 256, kernel_size=(1, 3), padding=(0, 1))   \n",
    "        self.e22 = nn.Conv2d(256, 256, kernel_size=(1, 3), padding=(0, 1))   \n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=(1, 2), stride=(1, 2))\n",
    "        # [128, 125] → [256, 125] → [256, 62]\n",
    "\n",
    "        # Third Level\n",
    "        self.e31 = nn.Conv2d(256, 512, kernel_size=(1, 3), padding=(0, 1))   \n",
    "        self.e32 = nn.Conv2d(512, 512, kernel_size=(1, 3), padding=(0, 1))   \n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=(1, 2), stride=(1, 2))        \n",
    "        # [256, 62] → [512, 62] → [512, 31]\n",
    "\n",
    "        # Bridge as MLP - with corrected dimensions based on actual tensor shape\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.bridge_mlp = nn.Sequential(\n",
    "            nn.Linear(512 * 1 * 3, 2048),  # Changed from 31 to 3\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(2048, 512 * 1 * 3),  # Changed from 31 to 3\n",
    "        )\n",
    "        self.unflatten = nn.Unflatten(1, (512, 1, 3))  # Changed from 31 to 3\n",
    "\n",
    "        # Decoder - Updated first upconv to match bridge output\n",
    "        self.upconv1 = nn.ConvTranspose2d(512, 512, kernel_size=(1, 2), stride=(1, 2))  # Changed from 1024 to 512\n",
    "        self.d11 = nn.Conv2d(1024, 512, kernel_size=(1, 3), padding=(0, 1))   \n",
    "        self.d12 = nn.Conv2d(512, 512, kernel_size=(1, 3), padding=(0, 1))    \n",
    "\n",
    "\n",
    "        self.upconv2 = nn.ConvTranspose2d(512, 256, kernel_size=(1, 2), stride=(1, 2))\n",
    "        self.d21 = nn.Conv2d(512, 256, kernel_size=(1, 3), padding=(0, 1))    \n",
    "        self.d22 = nn.Conv2d(256, 256, kernel_size=(1, 3), padding=(0, 1))    \n",
    "\n",
    "        self.upconv3 = nn.ConvTranspose2d(256, 128, kernel_size=(1, 2), stride=(1, 2))\n",
    "        self.d31 = nn.Conv2d(256, 128, kernel_size=(1, 3), padding=(0, 1))    \n",
    "        self.d32 = nn.Conv2d(128, 128, kernel_size=(1, 3), padding=(0, 1))    \n",
    "\n",
    "        # Output layer\n",
    "        self.outconv = nn.Conv2d(128, 102, kernel_size=(1, 1))                   \n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Reshape input: [batch, channels, time] -> [batch, 1, channels, time]\n",
    "        x = x.unsqueeze(1)\n",
    "        \n",
    "        # Encoder\n",
    "        xe11 = self.relu(self.e11(x))\n",
    "        xe12 = self.relu(self.e12(xe11))\n",
    "        xp1 = self.pool1(xe12)\n",
    "        xp1 = self.dropout(xp1)\n",
    "\n",
    "        xe21 = self.relu(self.e21(xp1))\n",
    "        xe22 = self.relu(self.e22(xe21))\n",
    "        xp2 = self.pool2(xe22)\n",
    "        xp2 = self.dropout(xp2)\n",
    "\n",
    "        xe31 = self.relu(self.e31(xp2))\n",
    "        xe32 = self.relu(self.e32(xe31))\n",
    "        xp3 = self.pool3(xe32)\n",
    "        xp3 = self.dropout(xp3)\n",
    "\n",
    "        # Bridge\n",
    "        print(f\"Before flatten shape: {xp3.shape}\")\n",
    "        xb = self.flatten(xp3)\n",
    "        print(f\"After flatten shape: {xb.shape}\")\n",
    "        xb = self.bridge_mlp(xb)\n",
    "        print(f\"After MLP shape: {xb.shape}\")\n",
    "        xb2 = self.unflatten(xb)\n",
    "        print(f\"After unflatten shape: {xb2.shape}\")\n",
    "        xb2 = self.dropout(xb2)\n",
    "\n",
    "\n",
    "        # Decoder with size matching\n",
    "        xu1 = self.upconv1(xb2)\n",
    "        xu1 = torch.nn.functional.pad(xu1, [0, xe32.size(-1) - xu1.size(-1), 0, 0])\n",
    "        xu11 = torch.cat([xu1, xe32], dim=1)\n",
    "        xd11 = self.relu(self.d11(xu11))\n",
    "        xd12 = self.relu(self.d12(xd11))\n",
    "        xd12 = self.dropout(xd12)\n",
    "\n",
    "        xu2 = self.upconv2(xd12)\n",
    "        xu2 = torch.nn.functional.pad(xu2, [0, xe22.size(-1) - xu2.size(-1), 0, 0])\n",
    "        xu22 = torch.cat([xu2, xe22], dim=1)\n",
    "        xd21 = self.relu(self.d21(xu22))\n",
    "        xd22 = self.relu(self.d22(xd21))\n",
    "        xd22 = self.dropout(xd22)\n",
    "\n",
    "        xu3 = self.upconv3(xd22)\n",
    "        xu3 = torch.nn.functional.pad(xu3, [0, xe12.size(-1) - xu3.size(-1), 0, 0])\n",
    "        xu33 = torch.cat([xu3, xe12], dim=1)\n",
    "        xd31 = self.relu(self.d31(xu33))\n",
    "        xd32 = self.relu(self.d32(xd31))\n",
    "\n",
    "        # Output layer\n",
    "        out = self.outconv(xd32)\n",
    "        \n",
    "        # Reshape output:  [batch, 102, 1, time] -> [batch, 102, time]\n",
    "        out = out.squeeze(2)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random array test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 77, 250])\n",
      "Before flatten shape: torch.Size([2, 512, 1, 3])\n",
      "After flatten shape: torch.Size([2, 1536])\n",
      "After MLP shape: torch.Size([2, 1536])\n",
      "After unflatten shape: torch.Size([2, 512, 1, 3])\n",
      "Output shape: torch.Size([2, 102, 24])\n"
     ]
    }
   ],
   "source": [
    "# Create test data\n",
    "batch_size = 2\n",
    "eeg_channels = 77\n",
    "time_points = 250\n",
    "\n",
    "test_data = torch.randn(batch_size, eeg_channels, time_points)\n",
    "print(f\"Input shape: {test_data.shape}\")\n",
    "\n",
    "# Initialize and test\n",
    "model = EEGtoMEGUNet2D()\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(test_data)\n",
    "    print(f\"Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "synaptech_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
