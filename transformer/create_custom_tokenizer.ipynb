{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from datasets import Dataset\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers_from_scratch import Transformer\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from rouge_score import rouge_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>French</th>\n",
       "      <th>Bassa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Il y avait dans le pays d'Uts un homme dont le...</td>\n",
       "      <td>Mut wada a bé yééne i loñ Us, jôl jé li bé le ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Et il lui naquit sept fils et trois filles;</td>\n",
       "      <td>A bééna bon bôlôm basaambok, ni bon bôda baa.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>et il possédait sept mille brebis, et trois mi...</td>\n",
       "      <td>A bééna ki 7 000 di mintômba, 3 000 di kamél, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Et ses fils allaient et faisaient un festin, c...</td>\n",
       "      <td>Hiki man wé nu munlôm a bééna yé ngéda i tégba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Et il arrivait que, quand les jours de festin ...</td>\n",
       "      <td>I ngéda ba bé ba mal i mangand ma, Hiôb a bé a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              French  \\\n",
       "0  Il y avait dans le pays d'Uts un homme dont le...   \n",
       "1        Et il lui naquit sept fils et trois filles;   \n",
       "2  et il possédait sept mille brebis, et trois mi...   \n",
       "3  Et ses fils allaient et faisaient un festin, c...   \n",
       "4  Et il arrivait que, quand les jours de festin ...   \n",
       "\n",
       "                                               Bassa  \n",
       "0  Mut wada a bé yééne i loñ Us, jôl jé li bé le ...  \n",
       "1      A bééna bon bôlôm basaambok, ni bon bôda baa.  \n",
       "2  A bééna ki 7 000 di mintômba, 3 000 di kamél, ...  \n",
       "3  Hiki man wé nu munlôm a bééna yé ngéda i tégba...  \n",
       "4  I ngéda ba bé ba mal i mangand ma, Hiôb a bé a...  "
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "file_path = \"src/sample_dataset_1.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method to train an encoder-decoder transformer model from scratch where the encoder processes French text and the decoder processes Bassa text. To do this, it is better to use separate tokenizers for each language. This way, each tokenizer can focus on the specific vocabulary and tokenization rules of its respective language, which can improve the efficiency and accuracy of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Create Separate Tokenizers\n",
    "We'll create separate tokenizers for French and Bassa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Save the French texts to a temporary file\n",
    "os.makedirs(\"temp\", exist_ok=True)\n",
    "french_texts_path = \"temp/french_texts.txt\"\n",
    "data['French'].to_csv(french_texts_path, index=False, header=False)\n",
    "\n",
    "# Save the Bassa texts to a temporary file\n",
    "bassa_texts_path = \"temp/bassa_texts.txt\"\n",
    "data['Bassa'].to_csv(bassa_texts_path, index=False, header=False)\n",
    "\n",
    "# Initialize and train the French tokenizer\n",
    "french_tokenizer = BertWordPieceTokenizer()\n",
    "french_tokenizer.train(files=[french_texts_path], vocab_size=30_000, min_frequency=2, special_tokens=[\n",
    "    \"[PAD]\",\n",
    "    \"[UNK]\",\n",
    "    \"[CLS]\",\n",
    "    \"[SEP]\",\n",
    "    \"[MASK]\",\n",
    "])\n",
    "os.makedirs(\"french_tokenizer\", exist_ok=True)\n",
    "french_tokenizer.save_model(\"french_tokenizer\")\n",
    "\n",
    "# Initialize and train the Bassa tokenizer\n",
    "bassa_tokenizer = BertWordPieceTokenizer()\n",
    "bassa_tokenizer.train(files=[bassa_texts_path], vocab_size=30_000, min_frequency=2, special_tokens=[\n",
    "    \"[PAD]\",\n",
    "    \"[UNK]\",\n",
    "    \"[CLS]\",\n",
    "    \"[SEP]\",\n",
    "    \"[MASK]\",\n",
    "])\n",
    "os.makedirs(\"bassa_tokenizer\", exist_ok=True)\n",
    "bassa_tokenizer.save_model(\"bassa_tokenizer\")\n",
    "\n",
    "# Load the tokenizers using the transformers library\n",
    "from transformers import BertTokenizerFast\n",
    "\n",
    "french_tokenizer = BertTokenizerFast.from_pretrained(\"french_tokenizer\")\n",
    "bassa_tokenizer = BertTokenizerFast.from_pretrained(\"bassa_tokenizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Prepare the Dataset\n",
    "\n",
    "We will prepare the datasets for training using the respective tokenizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and validation sets\n",
    "train_french, val_french, train_bassa, val_bassa = train_test_split(\n",
    "    data['French'], data['Bassa'], test_size=0.1, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6384     Car ainsi m'a dit le Seigneur: Va, place une s...\n",
       "9841     Car autrefois, aux jours de David et d'Asaph, ...\n",
       "1599     ils ne seront pas confus au mauvais temps, et ...\n",
       "3295     C'est ici mon repos à perpétuité; ici j'habite...\n",
       "13729    Dieu dit à Abraham: Quant à Saraï, ta femme, t...\n",
       "                               ...                        \n",
       "13818    Et Dieu lui dit en songe: Moi aussi je sais qu...\n",
       "5352     Et si un homme a emprunté une bête à son proch...\n",
       "10528    Moi, j'ai fait la terre, l'homme et la bête qu...\n",
       "3511     Animaux et tout le bétail, reptiles et oiseaux...\n",
       "17166    Élisée vint à Damas; et Ben-Hadad, roi de Syri...\n",
       "Name: French, Length: 2312, dtype: object"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_french"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6384     Inyule Yéhôva a nkal me le: “Ke téé mut faa, a...\n",
       "9841     Inyule i ngéda kôba, i dilo di David bo Asaf, ...\n",
       "1599     Ba ga wo bé nyuu i ngéda bikuu; ba ga bana nga...\n",
       "3295     “Linôyôi jem li i boga ni boga; m’a yééne+ mu,...\n",
       "13729    I mbus, Nyambe a kal Abraham le: “Inyu nwaa wo...\n",
       "                               ...                        \n",
       "13818    Bañga Nyambe a kal nye ikété eem le: “Me nyi l...\n",
       "5352     “Ibale mut a mpoo lém yak mut wé libôk, ndi lé...\n",
       "10528    ‘Men me bi hek hisi, bôt ba binam ni binuga bi...\n",
       "3511     a bé binuga bi bikai+ ni bilém gwobisôna, a bé...\n",
       "17166    Élisa a bi ke i Damaskô+ i ngéda Ben-Hadad,+ k...\n",
       "Name: Bassa, Length: 2312, dtype: object"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_bassa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to prepare dataset\n",
    "def tokenize_and_prepare_dataset(src_texts, tgt_texts, src_tokenizer, tgt_tokenizer):\n",
    "    src_encodings = src_tokenizer(src_texts.tolist(), truncation=True, padding=True, max_length=100, return_tensors=\"pt\")\n",
    "    tgt_encodings = tgt_tokenizer(tgt_texts.tolist(), truncation=True, padding=True, max_length=100, return_tensors=\"pt\")\n",
    "\n",
    "    dataset = TensorDataset(src_encodings['input_ids'], tgt_encodings['input_ids'])\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/15606 [2:52:42<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "# Tokenize and prepare training and validation datasets\n",
    "train_dataset = tokenize_and_prepare_dataset(train_french, train_bassa, french_tokenizer, bassa_tokenizer)\n",
    "val_dataset = tokenize_and_prepare_dataset(val_french, val_bassa, french_tokenizer, bassa_tokenizer)\n",
    "\n",
    "# DataLoader\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Train the Model\n",
    "Now, we need to set up the training loop to train your custom transformer model using the prepared data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Hyperparameters\n",
    "src_vocab_size = french_tokenizer.vocab_size\n",
    "trg_vocab_size = bassa_tokenizer.vocab_size\n",
    "# Define the padding token indices for the source and target tokenizers\n",
    "src_pad_idx = french_tokenizer.pad_token_id\n",
    "trg_pad_idx = bassa_tokenizer.pad_token_id\n",
    "embed_size = 256\n",
    "num_layers = 6\n",
    "forward_expansion = 4\n",
    "heads = 8\n",
    "dropout = 0.1\n",
    "max_length = 100\n",
    "learning_rate = 0.0003\n",
    "\n",
    "# Initialize the transformer model\n",
    "model = Transformer(src_vocab_size, trg_vocab_size, src_pad_idx=src_pad_idx, trg_pad_idx=trg_pad_idx, embed_size=embed_size, num_layers=num_layers, forward_expansion=forward_expansion, heads=heads, dropout=dropout, device=device, max_length=max_length).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Step [0/651], Loss: 9.0764\n",
      "Epoch [1/50], Step [100/651], Loss: 5.5660\n",
      "Epoch [1/50], Step [200/651], Loss: 5.5436\n",
      "Epoch [1/50], Step [300/651], Loss: 5.6159\n",
      "Epoch [1/50], Step [400/651], Loss: 5.3959\n",
      "Epoch [1/50], Step [500/651], Loss: 5.1963\n",
      "Epoch [1/50], Step [600/651], Loss: 4.8183\n",
      "Validation Loss after Epoch [1/50]: 4.7980\n",
      "Epoch [2/50], Step [0/651], Loss: 4.8736\n",
      "Epoch [2/50], Step [100/651], Loss: 4.7693\n",
      "Epoch [2/50], Step [200/651], Loss: 4.6657\n",
      "Epoch [2/50], Step [300/651], Loss: 4.7549\n",
      "Epoch [2/50], Step [400/651], Loss: 4.5263\n",
      "Epoch [2/50], Step [500/651], Loss: 4.4027\n",
      "Epoch [2/50], Step [600/651], Loss: 4.4354\n",
      "Validation Loss after Epoch [2/50]: 4.3628\n",
      "Epoch [3/50], Step [0/651], Loss: 4.3717\n",
      "Epoch [3/50], Step [100/651], Loss: 4.2544\n",
      "Epoch [3/50], Step [200/651], Loss: 4.2041\n",
      "Epoch [3/50], Step [300/651], Loss: 4.2838\n",
      "Epoch [3/50], Step [400/651], Loss: 4.2906\n",
      "Epoch [3/50], Step [500/651], Loss: 4.2283\n",
      "Epoch [3/50], Step [600/651], Loss: 4.1304\n",
      "Validation Loss after Epoch [3/50]: 4.0780\n",
      "Epoch [4/50], Step [0/651], Loss: 4.0797\n",
      "Epoch [4/50], Step [100/651], Loss: 4.0485\n",
      "Epoch [4/50], Step [200/651], Loss: 4.2112\n",
      "Epoch [4/50], Step [300/651], Loss: 4.1166\n",
      "Epoch [4/50], Step [400/651], Loss: 3.9593\n",
      "Epoch [4/50], Step [500/651], Loss: 4.0535\n",
      "Epoch [4/50], Step [600/651], Loss: 4.0278\n",
      "Validation Loss after Epoch [4/50]: 3.9018\n",
      "Epoch [5/50], Step [0/651], Loss: 4.0948\n",
      "Epoch [5/50], Step [100/651], Loss: 3.7278\n",
      "Epoch [5/50], Step [200/651], Loss: 3.8253\n",
      "Epoch [5/50], Step [300/651], Loss: 3.8200\n",
      "Epoch [5/50], Step [400/651], Loss: 4.0137\n",
      "Epoch [5/50], Step [500/651], Loss: 3.8384\n",
      "Epoch [5/50], Step [600/651], Loss: 3.8537\n",
      "Validation Loss after Epoch [5/50]: 3.7569\n",
      "Epoch [6/50], Step [0/651], Loss: 3.9526\n",
      "Epoch [6/50], Step [100/651], Loss: 3.8918\n",
      "Epoch [6/50], Step [200/651], Loss: 3.8036\n",
      "Epoch [6/50], Step [300/651], Loss: 3.7056\n",
      "Epoch [6/50], Step [400/651], Loss: 3.7559\n",
      "Epoch [6/50], Step [500/651], Loss: 3.6126\n",
      "Epoch [6/50], Step [600/651], Loss: 3.5284\n",
      "Validation Loss after Epoch [6/50]: 3.6430\n",
      "Epoch [7/50], Step [0/651], Loss: 3.7416\n",
      "Epoch [7/50], Step [100/651], Loss: 3.6780\n",
      "Epoch [7/50], Step [200/651], Loss: 3.4825\n",
      "Epoch [7/50], Step [300/651], Loss: 3.5222\n",
      "Epoch [7/50], Step [400/651], Loss: 3.7528\n",
      "Epoch [7/50], Step [500/651], Loss: 3.6109\n",
      "Epoch [7/50], Step [600/651], Loss: 3.6698\n",
      "Validation Loss after Epoch [7/50]: 3.5370\n",
      "Epoch [8/50], Step [0/651], Loss: 3.5724\n",
      "Epoch [8/50], Step [100/651], Loss: 3.6195\n",
      "Epoch [8/50], Step [200/651], Loss: 3.5729\n",
      "Epoch [8/50], Step [300/651], Loss: 3.4657\n",
      "Epoch [8/50], Step [400/651], Loss: 3.5657\n",
      "Epoch [8/50], Step [500/651], Loss: 3.5121\n",
      "Epoch [8/50], Step [600/651], Loss: 3.4397\n",
      "Validation Loss after Epoch [8/50]: 3.4352\n",
      "Epoch [9/50], Step [0/651], Loss: 3.3879\n",
      "Epoch [9/50], Step [100/651], Loss: 3.5580\n",
      "Epoch [9/50], Step [200/651], Loss: 3.5201\n",
      "Epoch [9/50], Step [300/651], Loss: 3.5848\n",
      "Epoch [9/50], Step [400/651], Loss: 3.3815\n",
      "Epoch [9/50], Step [500/651], Loss: 3.4294\n",
      "Epoch [9/50], Step [600/651], Loss: 3.4946\n",
      "Validation Loss after Epoch [9/50]: 3.3415\n",
      "Epoch [10/50], Step [0/651], Loss: 3.2248\n",
      "Epoch [10/50], Step [100/651], Loss: 3.3441\n",
      "Epoch [10/50], Step [200/651], Loss: 3.1749\n",
      "Epoch [10/50], Step [300/651], Loss: 3.2275\n",
      "Epoch [10/50], Step [400/651], Loss: 3.2387\n",
      "Epoch [10/50], Step [500/651], Loss: 3.2247\n",
      "Epoch [10/50], Step [600/651], Loss: 3.3872\n",
      "Validation Loss after Epoch [10/50]: 3.2681\n",
      "Epoch [11/50], Step [0/651], Loss: 3.1156\n",
      "Epoch [11/50], Step [100/651], Loss: 3.3723\n",
      "Epoch [11/50], Step [200/651], Loss: 3.3582\n",
      "Epoch [11/50], Step [300/651], Loss: 3.1195\n",
      "Epoch [11/50], Step [400/651], Loss: 3.2229\n",
      "Epoch [11/50], Step [500/651], Loss: 3.4977\n",
      "Epoch [11/50], Step [600/651], Loss: 3.2353\n",
      "Validation Loss after Epoch [11/50]: 3.2127\n",
      "Epoch [12/50], Step [0/651], Loss: 3.0411\n",
      "Epoch [12/50], Step [100/651], Loss: 3.3672\n",
      "Epoch [12/50], Step [200/651], Loss: 3.0897\n",
      "Epoch [12/50], Step [300/651], Loss: 3.1567\n",
      "Epoch [12/50], Step [400/651], Loss: 3.0452\n",
      "Epoch [12/50], Step [500/651], Loss: 3.2156\n",
      "Epoch [12/50], Step [600/651], Loss: 3.2047\n",
      "Validation Loss after Epoch [12/50]: 3.1489\n",
      "Epoch [13/50], Step [0/651], Loss: 3.1556\n",
      "Epoch [13/50], Step [100/651], Loss: 3.1344\n",
      "Epoch [13/50], Step [200/651], Loss: 3.1337\n",
      "Epoch [13/50], Step [300/651], Loss: 3.1816\n",
      "Epoch [13/50], Step [400/651], Loss: 3.0984\n",
      "Epoch [13/50], Step [500/651], Loss: 3.0143\n",
      "Epoch [13/50], Step [600/651], Loss: 3.2539\n",
      "Validation Loss after Epoch [13/50]: 3.0910\n",
      "Epoch [14/50], Step [0/651], Loss: 2.9757\n",
      "Epoch [14/50], Step [100/651], Loss: 3.1008\n",
      "Epoch [14/50], Step [200/651], Loss: 3.0114\n",
      "Epoch [14/50], Step [300/651], Loss: 3.0160\n",
      "Epoch [14/50], Step [400/651], Loss: 3.1046\n",
      "Epoch [14/50], Step [500/651], Loss: 3.0479\n",
      "Epoch [14/50], Step [600/651], Loss: 3.0016\n",
      "Validation Loss after Epoch [14/50]: 3.0369\n",
      "Epoch [15/50], Step [0/651], Loss: 2.8578\n",
      "Epoch [15/50], Step [100/651], Loss: 2.8150\n",
      "Epoch [15/50], Step [200/651], Loss: 2.8980\n",
      "Epoch [15/50], Step [300/651], Loss: 2.9933\n",
      "Epoch [15/50], Step [400/651], Loss: 3.0597\n",
      "Epoch [15/50], Step [500/651], Loss: 2.7914\n",
      "Epoch [15/50], Step [600/651], Loss: 2.7987\n",
      "Validation Loss after Epoch [15/50]: 3.0105\n",
      "Epoch [16/50], Step [0/651], Loss: 2.8478\n",
      "Epoch [16/50], Step [100/651], Loss: 2.7954\n",
      "Epoch [16/50], Step [200/651], Loss: 2.8712\n",
      "Epoch [16/50], Step [300/651], Loss: 2.8888\n",
      "Epoch [16/50], Step [400/651], Loss: 2.9220\n",
      "Epoch [16/50], Step [500/651], Loss: 2.9661\n",
      "Epoch [16/50], Step [600/651], Loss: 2.9409\n",
      "Validation Loss after Epoch [16/50]: 2.9607\n",
      "Epoch [17/50], Step [0/651], Loss: 2.8931\n",
      "Epoch [17/50], Step [100/651], Loss: 2.8595\n",
      "Epoch [17/50], Step [200/651], Loss: 2.7081\n",
      "Epoch [17/50], Step [300/651], Loss: 2.8565\n",
      "Epoch [17/50], Step [400/651], Loss: 2.6810\n",
      "Epoch [17/50], Step [500/651], Loss: 2.8497\n",
      "Epoch [17/50], Step [600/651], Loss: 3.0077\n",
      "Validation Loss after Epoch [17/50]: 2.9289\n",
      "Epoch [18/50], Step [0/651], Loss: 2.8042\n",
      "Epoch [18/50], Step [100/651], Loss: 2.6934\n",
      "Epoch [18/50], Step [200/651], Loss: 2.7974\n",
      "Epoch [18/50], Step [300/651], Loss: 2.9136\n",
      "Epoch [18/50], Step [400/651], Loss: 2.8485\n",
      "Epoch [18/50], Step [500/651], Loss: 2.7939\n",
      "Epoch [18/50], Step [600/651], Loss: 2.8852\n",
      "Validation Loss after Epoch [18/50]: 2.8896\n",
      "Epoch [19/50], Step [0/651], Loss: 2.8923\n",
      "Epoch [19/50], Step [100/651], Loss: 2.9722\n",
      "Epoch [19/50], Step [200/651], Loss: 2.6769\n",
      "Epoch [19/50], Step [300/651], Loss: 2.8002\n",
      "Epoch [19/50], Step [400/651], Loss: 2.8625\n",
      "Epoch [19/50], Step [500/651], Loss: 2.6595\n",
      "Epoch [19/50], Step [600/651], Loss: 2.8793\n",
      "Validation Loss after Epoch [19/50]: 2.8741\n",
      "Epoch [20/50], Step [0/651], Loss: 2.7615\n",
      "Epoch [20/50], Step [100/651], Loss: 2.8363\n",
      "Epoch [20/50], Step [200/651], Loss: 2.7017\n",
      "Epoch [20/50], Step [300/651], Loss: 2.8190\n",
      "Epoch [20/50], Step [400/651], Loss: 2.7077\n",
      "Epoch [20/50], Step [500/651], Loss: 2.7325\n",
      "Epoch [20/50], Step [600/651], Loss: 2.6381\n",
      "Validation Loss after Epoch [20/50]: 2.8288\n",
      "Epoch [21/50], Step [0/651], Loss: 2.6111\n",
      "Epoch [21/50], Step [100/651], Loss: 2.7209\n",
      "Epoch [21/50], Step [200/651], Loss: 2.7886\n",
      "Epoch [21/50], Step [300/651], Loss: 2.5623\n",
      "Epoch [21/50], Step [400/651], Loss: 2.6694\n",
      "Epoch [21/50], Step [500/651], Loss: 2.4506\n",
      "Epoch [21/50], Step [600/651], Loss: 2.6679\n",
      "Validation Loss after Epoch [21/50]: 2.8160\n",
      "Epoch [22/50], Step [0/651], Loss: 2.6059\n",
      "Epoch [22/50], Step [100/651], Loss: 2.6367\n",
      "Epoch [22/50], Step [200/651], Loss: 2.6184\n",
      "Epoch [22/50], Step [300/651], Loss: 2.7971\n",
      "Epoch [22/50], Step [400/651], Loss: 2.6553\n",
      "Epoch [22/50], Step [500/651], Loss: 2.6483\n",
      "Epoch [22/50], Step [600/651], Loss: 2.6148\n",
      "Validation Loss after Epoch [22/50]: 2.7897\n",
      "Epoch [23/50], Step [0/651], Loss: 2.4927\n",
      "Epoch [23/50], Step [100/651], Loss: 2.6448\n",
      "Epoch [23/50], Step [200/651], Loss: 2.7148\n",
      "Epoch [23/50], Step [300/651], Loss: 2.5233\n",
      "Epoch [23/50], Step [400/651], Loss: 2.7965\n",
      "Epoch [23/50], Step [500/651], Loss: 2.6079\n",
      "Epoch [23/50], Step [600/651], Loss: 2.6054\n",
      "Validation Loss after Epoch [23/50]: 2.7753\n",
      "Epoch [24/50], Step [0/651], Loss: 2.4788\n",
      "Epoch [24/50], Step [100/651], Loss: 2.6566\n",
      "Epoch [24/50], Step [200/651], Loss: 2.6236\n",
      "Epoch [24/50], Step [300/651], Loss: 2.6739\n",
      "Epoch [24/50], Step [400/651], Loss: 2.4717\n",
      "Epoch [24/50], Step [500/651], Loss: 2.6139\n",
      "Epoch [24/50], Step [600/651], Loss: 2.5847\n",
      "Validation Loss after Epoch [24/50]: 2.7442\n",
      "Epoch [25/50], Step [0/651], Loss: 2.4407\n",
      "Epoch [25/50], Step [100/651], Loss: 2.5771\n",
      "Epoch [25/50], Step [200/651], Loss: 2.6804\n",
      "Epoch [25/50], Step [300/651], Loss: 2.8487\n",
      "Epoch [25/50], Step [400/651], Loss: 2.4303\n",
      "Epoch [25/50], Step [500/651], Loss: 2.6061\n",
      "Epoch [25/50], Step [600/651], Loss: 2.7641\n",
      "Validation Loss after Epoch [25/50]: 2.7489\n",
      "Epoch [26/50], Step [0/651], Loss: 2.4816\n",
      "Epoch [26/50], Step [100/651], Loss: 2.5457\n",
      "Epoch [26/50], Step [200/651], Loss: 2.5583\n",
      "Epoch [26/50], Step [300/651], Loss: 2.6368\n",
      "Epoch [26/50], Step [400/651], Loss: 2.3675\n",
      "Epoch [26/50], Step [500/651], Loss: 2.4348\n",
      "Epoch [26/50], Step [600/651], Loss: 2.5621\n",
      "Validation Loss after Epoch [26/50]: 2.7185\n",
      "Epoch [27/50], Step [0/651], Loss: 2.5434\n",
      "Epoch [27/50], Step [100/651], Loss: 2.2562\n",
      "Epoch [27/50], Step [200/651], Loss: 2.3540\n",
      "Epoch [27/50], Step [300/651], Loss: 2.5689\n",
      "Epoch [27/50], Step [400/651], Loss: 2.5206\n",
      "Epoch [27/50], Step [500/651], Loss: 2.4580\n",
      "Epoch [27/50], Step [600/651], Loss: 2.5586\n",
      "Validation Loss after Epoch [27/50]: 2.7330\n",
      "Epoch [28/50], Step [0/651], Loss: 2.2595\n",
      "Epoch [28/50], Step [100/651], Loss: 2.5800\n",
      "Epoch [28/50], Step [200/651], Loss: 2.4885\n",
      "Epoch [28/50], Step [300/651], Loss: 2.5453\n",
      "Epoch [28/50], Step [400/651], Loss: 2.4383\n",
      "Epoch [28/50], Step [500/651], Loss: 2.2552\n",
      "Epoch [28/50], Step [600/651], Loss: 2.5214\n",
      "Validation Loss after Epoch [28/50]: 2.7161\n",
      "Epoch [29/50], Step [0/651], Loss: 2.4819\n",
      "Epoch [29/50], Step [100/651], Loss: 2.4084\n",
      "Epoch [29/50], Step [200/651], Loss: 2.5232\n",
      "Epoch [29/50], Step [300/651], Loss: 2.3501\n",
      "Epoch [29/50], Step [400/651], Loss: 2.3392\n",
      "Epoch [29/50], Step [500/651], Loss: 2.4785\n",
      "Epoch [29/50], Step [600/651], Loss: 2.5293\n",
      "Validation Loss after Epoch [29/50]: 2.6805\n",
      "Epoch [30/50], Step [0/651], Loss: 2.2631\n",
      "Epoch [30/50], Step [100/651], Loss: 2.4020\n",
      "Epoch [30/50], Step [200/651], Loss: 2.2538\n",
      "Epoch [30/50], Step [300/651], Loss: 2.4798\n",
      "Epoch [30/50], Step [400/651], Loss: 2.3810\n",
      "Epoch [30/50], Step [500/651], Loss: 2.3101\n",
      "Epoch [30/50], Step [600/651], Loss: 2.3048\n",
      "Validation Loss after Epoch [30/50]: 2.6708\n",
      "Epoch [31/50], Step [0/651], Loss: 2.2624\n",
      "Epoch [31/50], Step [100/651], Loss: 2.3804\n",
      "Epoch [31/50], Step [200/651], Loss: 2.2546\n",
      "Epoch [31/50], Step [300/651], Loss: 2.4451\n",
      "Epoch [31/50], Step [400/651], Loss: 2.4860\n",
      "Epoch [31/50], Step [500/651], Loss: 2.2713\n",
      "Epoch [31/50], Step [600/651], Loss: 2.5009\n",
      "Validation Loss after Epoch [31/50]: 2.6647\n",
      "Epoch [32/50], Step [0/651], Loss: 2.2321\n",
      "Epoch [32/50], Step [100/651], Loss: 2.2958\n",
      "Epoch [32/50], Step [200/651], Loss: 2.4750\n",
      "Epoch [32/50], Step [300/651], Loss: 2.3622\n",
      "Epoch [32/50], Step [400/651], Loss: 2.2842\n",
      "Epoch [32/50], Step [500/651], Loss: 2.3202\n",
      "Epoch [32/50], Step [600/651], Loss: 2.3348\n",
      "Validation Loss after Epoch [32/50]: 2.6699\n",
      "Epoch [33/50], Step [0/651], Loss: 2.3162\n",
      "Epoch [33/50], Step [100/651], Loss: 2.3789\n",
      "Epoch [33/50], Step [200/651], Loss: 2.4355\n",
      "Epoch [33/50], Step [300/651], Loss: 2.3660\n",
      "Epoch [33/50], Step [400/651], Loss: 2.4510\n",
      "Epoch [33/50], Step [500/651], Loss: 2.3426\n",
      "Epoch [33/50], Step [600/651], Loss: 2.4468\n",
      "Validation Loss after Epoch [33/50]: 2.6457\n",
      "Epoch [34/50], Step [0/651], Loss: 2.2709\n",
      "Epoch [34/50], Step [100/651], Loss: 2.3737\n",
      "Epoch [34/50], Step [200/651], Loss: 2.1768\n",
      "Epoch [34/50], Step [300/651], Loss: 2.3989\n",
      "Epoch [34/50], Step [400/651], Loss: 2.1106\n",
      "Epoch [34/50], Step [500/651], Loss: 2.3809\n",
      "Epoch [34/50], Step [600/651], Loss: 2.3562\n",
      "Validation Loss after Epoch [34/50]: 2.6509\n",
      "Epoch [35/50], Step [0/651], Loss: 2.3203\n",
      "Epoch [35/50], Step [100/651], Loss: 2.2351\n",
      "Epoch [35/50], Step [200/651], Loss: 2.2439\n",
      "Epoch [35/50], Step [300/651], Loss: 2.2588\n",
      "Epoch [35/50], Step [400/651], Loss: 2.2164\n",
      "Epoch [35/50], Step [500/651], Loss: 2.4960\n",
      "Epoch [35/50], Step [600/651], Loss: 2.2861\n",
      "Validation Loss after Epoch [35/50]: 2.6359\n",
      "Epoch [36/50], Step [0/651], Loss: 2.1247\n",
      "Epoch [36/50], Step [100/651], Loss: 2.1768\n",
      "Epoch [36/50], Step [200/651], Loss: 2.1754\n",
      "Epoch [36/50], Step [300/651], Loss: 2.1330\n",
      "Epoch [36/50], Step [400/651], Loss: 2.2893\n",
      "Epoch [36/50], Step [500/651], Loss: 2.4332\n",
      "Epoch [36/50], Step [600/651], Loss: 2.1677\n",
      "Validation Loss after Epoch [36/50]: 2.6351\n",
      "Epoch [37/50], Step [0/651], Loss: 2.1251\n",
      "Epoch [37/50], Step [100/651], Loss: 2.0569\n",
      "Epoch [37/50], Step [200/651], Loss: 2.2775\n",
      "Epoch [37/50], Step [300/651], Loss: 2.2854\n",
      "Epoch [37/50], Step [400/651], Loss: 1.9245\n",
      "Epoch [37/50], Step [500/651], Loss: 2.1388\n",
      "Epoch [37/50], Step [600/651], Loss: 2.2226\n",
      "Validation Loss after Epoch [37/50]: 2.6171\n",
      "Epoch [38/50], Step [0/651], Loss: 2.2482\n",
      "Epoch [38/50], Step [100/651], Loss: 2.2202\n",
      "Epoch [38/50], Step [200/651], Loss: 2.1720\n",
      "Epoch [38/50], Step [300/651], Loss: 2.2375\n",
      "Epoch [38/50], Step [400/651], Loss: 2.3269\n",
      "Epoch [38/50], Step [500/651], Loss: 2.2751\n",
      "Epoch [38/50], Step [600/651], Loss: 2.3773\n",
      "Validation Loss after Epoch [38/50]: 2.6168\n",
      "Epoch [39/50], Step [0/651], Loss: 2.0680\n",
      "Epoch [39/50], Step [100/651], Loss: 2.1278\n",
      "Epoch [39/50], Step [200/651], Loss: 2.1604\n",
      "Epoch [39/50], Step [300/651], Loss: 2.2817\n",
      "Epoch [39/50], Step [400/651], Loss: 2.2236\n",
      "Epoch [39/50], Step [500/651], Loss: 2.3737\n",
      "Epoch [39/50], Step [600/651], Loss: 2.3367\n",
      "Validation Loss after Epoch [39/50]: 2.6029\n",
      "Epoch [40/50], Step [0/651], Loss: 2.0069\n",
      "Epoch [40/50], Step [100/651], Loss: 2.2227\n",
      "Epoch [40/50], Step [200/651], Loss: 2.1003\n",
      "Epoch [40/50], Step [300/651], Loss: 2.0602\n",
      "Epoch [40/50], Step [400/651], Loss: 2.1817\n",
      "Epoch [40/50], Step [500/651], Loss: 2.3768\n",
      "Epoch [40/50], Step [600/651], Loss: 2.1665\n",
      "Validation Loss after Epoch [40/50]: 2.6039\n",
      "Epoch [41/50], Step [0/651], Loss: 2.1353\n",
      "Epoch [41/50], Step [100/651], Loss: 2.1328\n",
      "Epoch [41/50], Step [200/651], Loss: 2.1365\n",
      "Epoch [41/50], Step [300/651], Loss: 2.0882\n",
      "Epoch [41/50], Step [400/651], Loss: 2.0947\n",
      "Epoch [41/50], Step [500/651], Loss: 2.2056\n",
      "Epoch [41/50], Step [600/651], Loss: 2.1099\n",
      "Validation Loss after Epoch [41/50]: 2.6006\n",
      "Epoch [42/50], Step [0/651], Loss: 2.1046\n",
      "Epoch [42/50], Step [100/651], Loss: 2.3263\n",
      "Epoch [42/50], Step [200/651], Loss: 2.0628\n",
      "Epoch [42/50], Step [300/651], Loss: 1.9712\n",
      "Epoch [42/50], Step [400/651], Loss: 2.0487\n",
      "Epoch [42/50], Step [500/651], Loss: 2.0958\n",
      "Epoch [42/50], Step [600/651], Loss: 2.2332\n",
      "Validation Loss after Epoch [42/50]: 2.5908\n",
      "Epoch [43/50], Step [0/651], Loss: 2.0544\n",
      "Epoch [43/50], Step [100/651], Loss: 2.0640\n",
      "Epoch [43/50], Step [200/651], Loss: 2.2678\n",
      "Epoch [43/50], Step [300/651], Loss: 1.9426\n",
      "Epoch [43/50], Step [400/651], Loss: 2.2595\n",
      "Epoch [43/50], Step [500/651], Loss: 2.1922\n",
      "Epoch [43/50], Step [600/651], Loss: 2.2283\n",
      "Validation Loss after Epoch [43/50]: 2.5929\n",
      "Epoch [44/50], Step [0/651], Loss: 1.9551\n",
      "Epoch [44/50], Step [100/651], Loss: 2.1762\n",
      "Epoch [44/50], Step [200/651], Loss: 2.0602\n",
      "Epoch [44/50], Step [300/651], Loss: 2.0360\n",
      "Epoch [44/50], Step [400/651], Loss: 2.0742\n",
      "Epoch [44/50], Step [500/651], Loss: 1.9464\n",
      "Epoch [44/50], Step [600/651], Loss: 2.2699\n",
      "Validation Loss after Epoch [44/50]: 2.5961\n",
      "Epoch [45/50], Step [0/651], Loss: 1.9910\n",
      "Epoch [45/50], Step [100/651], Loss: 2.1475\n",
      "Epoch [45/50], Step [200/651], Loss: 1.9997\n",
      "Epoch [45/50], Step [300/651], Loss: 2.1473\n",
      "Epoch [45/50], Step [400/651], Loss: 2.2050\n",
      "Epoch [45/50], Step [500/651], Loss: 2.2091\n",
      "Epoch [45/50], Step [600/651], Loss: 1.9672\n",
      "Validation Loss after Epoch [45/50]: 2.5770\n",
      "Epoch [46/50], Step [0/651], Loss: 1.9562\n",
      "Epoch [46/50], Step [100/651], Loss: 1.9823\n",
      "Epoch [46/50], Step [200/651], Loss: 2.0406\n",
      "Epoch [46/50], Step [300/651], Loss: 2.0308\n",
      "Epoch [46/50], Step [400/651], Loss: 2.1572\n",
      "Epoch [46/50], Step [500/651], Loss: 2.2009\n",
      "Epoch [46/50], Step [600/651], Loss: 2.0405\n",
      "Validation Loss after Epoch [46/50]: 2.5839\n",
      "Epoch [47/50], Step [0/651], Loss: 2.0626\n",
      "Epoch [47/50], Step [100/651], Loss: 2.1152\n",
      "Epoch [47/50], Step [200/651], Loss: 2.0115\n",
      "Epoch [47/50], Step [300/651], Loss: 2.0347\n",
      "Epoch [47/50], Step [400/651], Loss: 1.9327\n",
      "Epoch [47/50], Step [500/651], Loss: 2.1041\n",
      "Epoch [47/50], Step [600/651], Loss: 2.0640\n",
      "Validation Loss after Epoch [47/50]: 2.5693\n",
      "Epoch [48/50], Step [0/651], Loss: 1.8423\n",
      "Epoch [48/50], Step [100/651], Loss: 1.9512\n",
      "Epoch [48/50], Step [200/651], Loss: 1.9931\n",
      "Epoch [48/50], Step [300/651], Loss: 2.0221\n",
      "Epoch [48/50], Step [400/651], Loss: 2.0208\n",
      "Epoch [48/50], Step [500/651], Loss: 2.0544\n",
      "Epoch [48/50], Step [600/651], Loss: 2.1018\n",
      "Validation Loss after Epoch [48/50]: 2.5712\n",
      "Epoch [49/50], Step [0/651], Loss: 1.9469\n",
      "Epoch [49/50], Step [100/651], Loss: 1.8781\n",
      "Epoch [49/50], Step [200/651], Loss: 1.9907\n",
      "Epoch [49/50], Step [300/651], Loss: 2.0593\n",
      "Epoch [49/50], Step [400/651], Loss: 2.1533\n",
      "Epoch [49/50], Step [500/651], Loss: 2.1243\n",
      "Epoch [49/50], Step [600/651], Loss: 1.9773\n",
      "Validation Loss after Epoch [49/50]: 2.5675\n",
      "Epoch [50/50], Step [0/651], Loss: 1.9106\n",
      "Epoch [50/50], Step [100/651], Loss: 2.1012\n",
      "Epoch [50/50], Step [200/651], Loss: 1.9140\n",
      "Epoch [50/50], Step [300/651], Loss: 1.9373\n",
      "Epoch [50/50], Step [400/651], Loss: 2.0968\n",
      "Epoch [50/50], Step [500/651], Loss: 2.1067\n",
      "Epoch [50/50], Step [600/651], Loss: 2.0846\n",
      "Validation Loss after Epoch [50/50]: 2.5767\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch_idx, (src, trg) in enumerate(train_loader):\n",
    "        src, trg = src.to(device), trg.to(device)\n",
    "        trg_input = trg[:, :-1]\n",
    "        trg_target = trg[:, 1:]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, trg_input)\n",
    "        \n",
    "        # Reshape output and target to calculate loss\n",
    "        output = output.reshape(-1, output.shape[2])\n",
    "        trg_target = trg_target.reshape(-1)\n",
    "        \n",
    "        loss = criterion(output, trg_target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{batch_idx}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for src, trg in val_loader:\n",
    "            src, trg = src.to(device), trg.to(device)\n",
    "            trg_input = trg[:, :-1]\n",
    "            trg_target = trg[:, 1:]\n",
    "\n",
    "            output = model(src, trg_input)\n",
    "            output = output.reshape(-1, output.shape[2])\n",
    "            trg_target = trg_target.reshape(-1)\n",
    "\n",
    "            loss = criterion(output, trg_target)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    print(f\"Validation Loss after Epoch [{epoch+1}/{num_epochs}]: {val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bassa_tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_pad_idx = french_tokenizer.pad_token_id\n",
    "trg_pad_idx = bassa_tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to transformer_model.pth\n",
      "Model loaded from transformer_model.pth\n"
     ]
    }
   ],
   "source": [
    "# Save the model\n",
    "model_save_path = \"transformer_model.pth\"\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "print(f\"Model saved to {model_save_path}\")\n",
    "\n",
    "# Load the model\n",
    "loaded_model = Transformer(src_vocab_size, trg_vocab_size, src_pad_idx, trg_pad_idx, embed_size=256, num_layers=6, forward_expansion=4, heads=8, dropout=0.1, device=device, max_length=100).to(device)\n",
    "loaded_model.load_state_dict(torch.load(model_save_path))\n",
    "print(f\"Model loaded from {model_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentence(model, sentence, src_tokenizer, tgt_tokenizer, device, max_length=50):\n",
    "    src_tokens = src_tokenizer(sentence, return_tensors=\"pt\", max_length=max_length, truncation=True, padding=\"max_length\")\n",
    "    src_tokens = src_tokens['input_ids'].to(device)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Generate the source mask\n",
    "        src_mask = model.make_src_mask(src_tokens)\n",
    "        \n",
    "        # Encode the source tokens\n",
    "        enc_src = model.encoder(src_tokens, src_mask)\n",
    "        \n",
    "        # Prepare the initial target input token ([CLS] token)\n",
    "        tgt_tokens = torch.tensor([[tgt_tokenizer.cls_token_id]], dtype=torch.long).to(device)\n",
    "        \n",
    "        for _ in range(max_length):\n",
    "            # Generate the target mask\n",
    "            trg_mask = model.make_trg_mask(tgt_tokens).to(device)\n",
    "            \n",
    "            # Decode the current target tokens\n",
    "            output = model.decoder(tgt_tokens, enc_src, src_mask, trg_mask)\n",
    "            \n",
    "            # Get the last token's logits and apply softmax to get probabilities\n",
    "            preds = output[:, -1, :].softmax(dim=-1)\n",
    "            \n",
    "            # Get the token ID with the highest probability\n",
    "            next_token = preds.argmax(1).unsqueeze(0)\n",
    "            \n",
    "            # Concatenate the predicted token to the target tokens\n",
    "            tgt_tokens = torch.cat((tgt_tokens, next_token), dim=1)\n",
    "            \n",
    "            # Stop if the end token is generated\n",
    "            if next_token.item() == tgt_tokenizer.sep_token_id:\n",
    "                break\n",
    "        \n",
    "    # Decode the token IDs to get the translated sentence\n",
    "    translated_sentence = tgt_tokenizer.decode(tgt_tokens.squeeze().tolist(), skip_special_tokens=True)\n",
    "    return translated_sentence\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6384     Car ainsi m'a dit le Seigneur: Va, place une s...\n",
       "9841     Car autrefois, aux jours de David et d'Asaph, ...\n",
       "1599     ils ne seront pas confus au mauvais temps, et ...\n",
       "3295     C'est ici mon repos à perpétuité; ici j'habite...\n",
       "13729    Dieu dit à Abraham: Quant à Saraï, ta femme, t...\n",
       "                               ...                        \n",
       "13818    Et Dieu lui dit en songe: Moi aussi je sais qu...\n",
       "5352     Et si un homme a emprunté une bête à son proch...\n",
       "10528    Moi, j'ai fait la terre, l'homme et la bête qu...\n",
       "3511     Animaux et tout le bétail, reptiles et oiseaux...\n",
       "17166    Élisée vint à Damas; et Ben-Hadad, roi de Syri...\n",
       "Name: French, Length: 2312, dtype: object"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_french"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6384     Inyule Yéhôva a nkal me le: “Ke téé mut faa, a...\n",
       "9841     Inyule i ngéda kôba, i dilo di David bo Asaf, ...\n",
       "1599     Ba ga wo bé nyuu i ngéda bikuu; ba ga bana nga...\n",
       "3295     “Linôyôi jem li i boga ni boga; m’a yééne+ mu,...\n",
       "13729    I mbus, Nyambe a kal Abraham le: “Inyu nwaa wo...\n",
       "                               ...                        \n",
       "13818    Bañga Nyambe a kal nye ikété eem le: “Me nyi l...\n",
       "5352     “Ibale mut a mpoo lém yak mut wé libôk, ndi lé...\n",
       "10528    ‘Men me bi hek hisi, bôt ba binam ni binuga bi...\n",
       "3511     a bé binuga bi bikai+ ni bilém gwobisôna, a bé...\n",
       "17166    Élisa a bi ke i Damaskô+ i ngéda Ben-Hadad,+ k...\n",
       "Name: Bassa, Length: 2312, dtype: object"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_bassa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated Sentence: yehova a konde podos satan, a kal nye le :\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Assuming the model and tokenizers are already initialized and trained\n",
    "# sentence = \"Your input sentence in French\"\n",
    "#sentence = \"Bonjour tout le monde\"\n",
    "sentence = \"Et l'Éternel dit à Satan\"\n",
    "translated_sentence = translate_sentence(loaded_model, sentence, french_tokenizer, bassa_tokenizer, device)\n",
    "print(f\"Translated Sentence: {translated_sentence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated Sentence: kegla, kegla, ba ye kegla ; ba wok kegla, ba wok ki ngeda yosona.\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Du matin au soir, ils sont frappés; ils périssent pour toujours sans qu'on y fasse attention.\"\n",
    "translated_sentence = translate_sentence(loaded_model, sentence, french_tokenizer, bassa_tokenizer, device)\n",
    "print(f\"Translated Sentence: {translated_sentence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Inyule Yéhôva a nkal me le\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated Sentence: inyule yehova a bi kal me le :\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Car ainsi m'a dit le Seigneur\"\n",
    "translated_sentence = translate_sentence(loaded_model, sentence, french_tokenizer, bassa_tokenizer, device)\n",
    "print(f\"Translated Sentence: {translated_sentence}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def evaluate_bleu(model, data_loader, src_tokenizer, tgt_tokenizer, device, max_length=50):\n",
    "    model.eval()\n",
    "    references = []\n",
    "    hypotheses = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for src, tgt in data_loader:\n",
    "            src, tgt = src.to(device), tgt.to(device)\n",
    "\n",
    "            for i in range(src.size(0)):\n",
    "                src_sentence = src[i].unsqueeze(0)\n",
    "                tgt_sentence = tgt[i].unsqueeze(0)\n",
    "                tgt_input = tgt_sentence[:, :-1]\n",
    "\n",
    "                src_mask = model.make_src_mask(src_sentence)\n",
    "                enc_src = model.encoder(src_sentence, src_mask)\n",
    "                tgt_tokens = torch.tensor([[tgt_tokenizer.cls_token_id]], dtype=torch.long).to(device)\n",
    "\n",
    "                for _ in range(max_length):\n",
    "                    trg_mask = model.make_trg_mask(tgt_tokens).to(device)\n",
    "                    output = model.decoder(tgt_tokens, enc_src, src_mask, trg_mask)\n",
    "                    preds = output[:, -1, :].softmax(dim=-1)\n",
    "                    next_token = preds.argmax(1).unsqueeze(0)\n",
    "                    tgt_tokens = torch.cat((tgt_tokens, next_token), dim=1)\n",
    "                    if next_token.item() == tgt_tokenizer.sep_token_id:\n",
    "                        break\n",
    "\n",
    "                predicted_sentence = tgt_tokenizer.decode(tgt_tokens.squeeze().tolist(), skip_special_tokens=True)\n",
    "                reference_sentence = tgt_tokenizer.decode(tgt_input.squeeze().tolist(), skip_special_tokens=True)\n",
    "\n",
    "                references.append([reference_sentence.split()])\n",
    "                hypotheses.append(predicted_sentence.split())\n",
    "\n",
    "    # Calculate BLEU score\n",
    "    smoothing = SmoothingFunction().method4\n",
    "    bleu_scores = [sentence_bleu(ref, hyp, smoothing_function=smoothing) for ref, hyp in zip(references, hypotheses)]\n",
    "    average_bleu = sum(bleu_scores) / len(bleu_scores)\n",
    "    return average_bleu\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU Score: 0.1148\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "bleu_score = evaluate_bleu(loaded_model, val_loader, french_tokenizer, bassa_tokenizer, device)\n",
    "print(f\"BLEU Score: {bleu_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "\n",
    "def evaluate_rouge(model, data_loader, src_tokenizer, tgt_tokenizer, device, max_length=50):\n",
    "    model.eval()\n",
    "    references = []\n",
    "    hypotheses = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for src, tgt in data_loader:\n",
    "            src, tgt = src.to(device), tgt.to(device)\n",
    "\n",
    "            for i in range(src.size(0)):\n",
    "                src_sentence = src[i].unsqueeze(0)\n",
    "                tgt_sentence = tgt[i].unsqueeze(0)\n",
    "                tgt_input = tgt_sentence[:, :-1]\n",
    "\n",
    "                src_mask = model.make_src_mask(src_sentence)\n",
    "                enc_src = model.encoder(src_sentence, src_mask)\n",
    "                tgt_tokens = torch.tensor([[tgt_tokenizer.cls_token_id]], dtype=torch.long).to(device)\n",
    "\n",
    "                for _ in range(max_length):\n",
    "                    trg_mask = model.make_trg_mask(tgt_tokens).to(device)\n",
    "                    output = model.decoder(tgt_tokens, enc_src, src_mask, trg_mask)\n",
    "                    preds = output[:, -1, :].softmax(dim=-1)\n",
    "                    next_token = preds.argmax(1).unsqueeze(0)\n",
    "                    tgt_tokens = torch.cat((tgt_tokens, next_token), dim=1)\n",
    "                    if next_token.item() == tgt_tokenizer.sep_token_id:\n",
    "                        break\n",
    "\n",
    "                predicted_sentence = tgt_tokenizer.decode(tgt_tokens.squeeze().tolist(), skip_special_tokens=True)\n",
    "                reference_sentence = tgt_tokenizer.decode(tgt_input.squeeze().tolist(), skip_special_tokens=True)\n",
    "\n",
    "                references.append(reference_sentence)\n",
    "                hypotheses.append(predicted_sentence)\n",
    "\n",
    "    # Calculate ROUGE score\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
    "    scores = [scorer.score(ref, hyp) for ref, hyp in zip(references, hypotheses)]\n",
    "    average_rouge1 = sum(score['rouge1'].fmeasure for score in scores) / len(scores)\n",
    "    average_rougeL = sum(score['rougeL'].fmeasure for score in scores) / len(scores)\n",
    "    return average_rouge1, average_rougeL\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE-1 Score: 0.4284\n",
      "ROUGE-L Score: 0.3626\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "rouge1, rougeL = evaluate_rouge(loaded_model, val_loader, french_tokenizer, bassa_tokenizer, device)\n",
    "print(f\"ROUGE-1 Score: {rouge1:.4f}\")\n",
    "print(f\"ROUGE-L Score: {rougeL:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BLEU Score (Bilingual Evaluation Understudy)\n",
    "\n",
    "**BLEU** is a precision-oriented metric that measures how many words (n-grams) in the candidate translation appear in the reference translation. It's widely used for evaluating machine translation models.\n",
    "\n",
    "#### Key Concepts\n",
    "\n",
    "1. **N-grams**: Sequences of N words. BLEU typically uses 1-grams (individual words) to 4-grams.\n",
    "2. **Precision**: The fraction of n-grams in the candidate translation that are also in the reference translation.\n",
    "3. **Brevity Penalty**: A penalty applied to short translations to avoid the model generating very short translations that would have high precision but miss a lot of content.\n",
    "\n",
    "#### Calculation\n",
    "\n",
    "1. **N-gram Precision**: Compute precision for 1-gram, 2-gram, ..., up to N-gram.\n",
    "2. **Geometric Mean**: Take the geometric mean of the n-gram precisions.\n",
    "3. **Brevity Penalty**: Apply the brevity penalty to the geometric mean to get the final BLEU score.\n",
    "\n",
    "#### Formula\n",
    "\n",
    "$$ \\text{BLEU} = \\text{BP} \\times \\exp \\left( \\sum_{n=1}^{N} w_n \\log p_n \\right) $$\n",
    "\n",
    "Where:\n",
    "- $ \\text{BP} $ is the brevity penalty.\n",
    "- $ w_n $ is the weight for n-grams (usually uniform weights like $ w_n = \\frac{1}{N} $).\n",
    "- $ p_n $ is the precision for n-grams.\n",
    "\n",
    "#### Example\n",
    "\n",
    "For a 1-gram BLEU score:\n",
    "- Candidate sentence: \"The cat is on the mat.\"\n",
    "- Reference sentence: \"There is a cat on the mat.\"\n",
    "\n",
    "1. Extract 1-grams: \"The\", \"cat\", \"is\", \"on\", \"the\", \"mat\".\n",
    "2. Calculate precision: 5 out of 6 1-grams in the candidate sentence are in the reference sentence.\n",
    "3. Apply brevity penalty if necessary.\n",
    "4. Compute BLEU score.\n",
    "\n",
    "### ROUGE Score (Recall-Oriented Understudy for Gisting Evaluation)\n",
    "\n",
    "**ROUGE** is a recall-oriented metric that measures the overlap of n-grams between the candidate summary and the reference summary. It's widely used for evaluating summarization models.\n",
    "\n",
    "#### Key Concepts\n",
    "\n",
    "1. **N-grams**: Sequences of N words. ROUGE-N (e.g., ROUGE-1, ROUGE-2) measures overlap of 1-grams, 2-grams, etc.\n",
    "2. **Recall**: The fraction of n-grams in the reference summary that are also in the candidate summary.\n",
    "3. **F-Measure**: The harmonic mean of precision and recall, providing a balanced measure.\n",
    "\n",
    "#### Types of ROUGE\n",
    "\n",
    "1. **ROUGE-N**: Measures the overlap of n-grams.\n",
    "2. **ROUGE-L**: Measures the longest common subsequence (LCS).\n",
    "3. **ROUGE-S**: Measures the overlap of skip-bigrams (pairs of words allowing gaps).\n",
    "\n",
    "#### Calculation\n",
    "\n",
    "1. **N-gram Overlap**: Compute the number of n-grams in the reference that appear in the candidate.\n",
    "2. **Recall**: Calculate recall based on the n-gram overlap.\n",
    "3. **Precision**: Calculate precision based on the n-gram overlap.\n",
    "4. **F-Measure**: Compute the F-measure as the harmonic mean of precision and recall.\n",
    "\n",
    "#### Formula\n",
    "\n",
    "For ROUGE-N:\n",
    "\n",
    "\\$$\\text{ROUGE-N} = \\frac{\\sum_{S \\in \\{\\text{References}\\}} \\sum_{gram_n \\in S} \\min(\\text{Count}_\\text{match}(gram_n), \\text{Count}(gram_n))}{\\sum_{S \\in \\{\\text{References}\\}} \\sum_{gram_n \\in S} \\text{Count}(gram_n)} $$\n",
    "\n",
    "Where:\n",
    "- $ \\text{Count}_\\text{match}(gram_n) $ is the count of n-grams that match in the candidate and reference.\n",
    "- $ \\text{Count}(gram_n) $ is the count of n-grams in the reference.\n",
    "\n",
    "#### Example\n",
    "\n",
    "For a ROUGE-1 score:\n",
    "- Candidate summary: \"The cat is on the mat.\"\n",
    "- Reference summary: \"There is a cat on the mat.\"\n",
    "\n",
    "1. Extract 1-grams: \"The\", \"cat\", \"is\", \"on\", \"the\", \"mat\".\n",
    "2. Calculate recall: 5 out of 7 1-grams in the reference summary are in the candidate summary.\n",
    "3. Calculate precision: 5 out of 6 1-grams in the candidate summary are in the reference summary.\n",
    "4. Compute F-measure as the harmonic mean of precision and recall.\n",
    "\n",
    "### Summary\n",
    "\n",
    "- **BLEU**: Precision-oriented, commonly used for machine translation, measures how much of the candidate is in the reference.\n",
    "- **ROUGE**: Recall-oriented, commonly used for summarization, measures how much of the reference is in the candidate.\n",
    "\n",
    "Both metrics provide insights into the quality of generated text compared to reference text, each with its own focus and method of calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both BLEU and ROUGE can be used for evaluating translation tasks, but they have different focuses and characteristics:\n",
    "\n",
    "- **BLEU (Bilingual Evaluation Understudy)**: It is more commonly used for machine translation tasks. BLEU emphasizes precision, which means it measures how many of the n-grams in the candidate translation are also in the reference translation. BLEU is well-suited for translation because it considers the order of words, which is important in language translation.\n",
    "\n",
    "- **ROUGE (Recall-Oriented Understudy for Gisting Evaluation)**: It is commonly used for summarization tasks but can also be used for translation. ROUGE emphasizes recall, meaning it measures how many of the n-grams in the reference translation are also in the candidate translation. ROUGE can be useful in translation tasks to some extent, especially in cases where capturing all the important content is crucial.\n",
    "\n",
    "### When to Use BLEU\n",
    "\n",
    "- If you are interested in how precise your translations are, i.e., how many of the n-grams in the candidate translation match the reference translation.\n",
    "- BLEU is widely accepted and used in the machine translation community.\n",
    "\n",
    "### When to Use ROUGE\n",
    "\n",
    "- If you want to measure recall, i.e., how much of the reference translation is covered by the candidate translation.\n",
    "- ROUGE might be less common for translation but can provide additional insights.\n",
    "\n",
    "### Combined Approach\n",
    "\n",
    "You can use both BLEU and ROUGE to get a more comprehensive evaluation. BLEU will give you an idea of precision, while ROUGE can provide insights into recall and the coverage of the reference translation.\n",
    "\n",
    "### Implementing BLEU and ROUGE for Your Translation Task\n",
    "\n",
    "Here’s how you can implement both BLEU and ROUGE to evaluate your French-to-Bassa translation model:\n",
    "\n",
    "#### BLEU Evaluation\n",
    "\n",
    "```python\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "def evaluate_bleu(model, data_loader, src_tokenizer, tgt_tokenizer, device, max_length=50):\n",
    "    model.eval()\n",
    "    references = []\n",
    "    hypotheses = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for src, tgt in data_loader:\n",
    "            src, tgt = src.to(device), tgt.to(device)\n",
    "\n",
    "            for i in range(src.size(0)):\n",
    "                src_sentence = src[i].unsqueeze(0)\n",
    "                tgt_sentence = tgt[i].unsqueeze(0)\n",
    "                tgt_input = tgt_sentence[:, :-1]\n",
    "\n",
    "                src_mask = model.make_src_mask(src_sentence)\n",
    "                enc_src = model.encoder(src_sentence, src_mask)\n",
    "                tgt_tokens = torch.tensor([[tgt_tokenizer.cls_token_id]], dtype=torch.long).to(device)\n",
    "\n",
    "                for _ in range(max_length):\n",
    "                    trg_mask = model.make_trg_mask(tgt_tokens).to(device)\n",
    "                    output = model.decoder(tgt_tokens, enc_src, src_mask, trg_mask)\n",
    "                    preds = output[:, -1, :].softmax(dim=-1)\n",
    "                    next_token = preds.argmax(1).unsqueeze(0)\n",
    "                    tgt_tokens = torch.cat((tgt_tokens, next_token), dim=1)\n",
    "                    if next_token.item() == tgt_tokenizer.sep_token_id:\n",
    "                        break\n",
    "\n",
    "                predicted_sentence = tgt_tokenizer.decode(tgt_tokens.squeeze().tolist(), skip_special_tokens=True)\n",
    "                reference_sentence = tgt_tokenizer.decode(tgt_input.squeeze().tolist(), skip_special_tokens=True)\n",
    "\n",
    "                references.append([reference_sentence.split()])\n",
    "                hypotheses.append(predicted_sentence.split())\n",
    "\n",
    "    # Calculate BLEU score\n",
    "    smoothing = SmoothingFunction().method4\n",
    "    bleu_scores = [sentence_bleu(ref, hyp, smoothing_function=smoothing) for ref, hyp in zip(references, hypotheses)]\n",
    "    average_bleu = sum(bleu_scores) / len(bleu_scores)\n",
    "    return average_bleu\n",
    "\n",
    "# Example usage\n",
    "bleu_score = evaluate_bleu(loaded_model, val_loader, french_tokenizer, bassa_tokenizer, device)\n",
    "print(f\"BLEU Score: {bleu_score:.4f}\")\n",
    "```\n",
    "\n",
    "#### ROUGE Evaluation\n",
    "\n",
    "```python\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "def evaluate_rouge(model, data_loader, src_tokenizer, tgt_tokenizer, device, max_length=50):\n",
    "    model.eval()\n",
    "    references = []\n",
    "    hypotheses = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for src, tgt in data_loader:\n",
    "            src, tgt = src.to(device), tgt.to(device)\n",
    "\n",
    "            for i in range(src.size(0)):\n",
    "                src_sentence = src[i].unsqueeze(0)\n",
    "                tgt_sentence = tgt[i].unsqueeze(0)\n",
    "                tgt_input = tgt_sentence[:, :-1]\n",
    "\n",
    "                src_mask = model.make_src_mask(src_sentence)\n",
    "                enc_src = model.encoder(src_sentence, src_mask)\n",
    "                tgt_tokens = torch.tensor([[tgt_tokenizer.cls_token_id]], dtype=torch.long).to(device)\n",
    "\n",
    "                for _ in range(max_length):\n",
    "                    trg_mask = model.make_trg_mask(tgt_tokens).to(device)\n",
    "                    output = model.decoder(tgt_tokens, enc_src, src_mask, trg_mask)\n",
    "                    preds = output[:, -1, :].softmax(dim=-1)\n",
    "                    next_token = preds.argmax(1).unsqueeze(0)\n",
    "                    tgt_tokens = torch.cat((tgt_tokens, next_token), dim=1)\n",
    "                    if next_token.item() == tgt_tokenizer.sep_token_id:\n",
    "                        break\n",
    "\n",
    "                predicted_sentence = tgt_tokenizer.decode(tgt_tokens.squeeze().tolist(), skip_special_tokens=True)\n",
    "                reference_sentence = tgt_tokenizer.decode(tgt_input.squeeze().tolist(), skip_special_tokens=True)\n",
    "\n",
    "                references.append(reference_sentence)\n",
    "                hypotheses.append(predicted_sentence)\n",
    "\n",
    "    # Calculate ROUGE score\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
    "    scores = [scorer.score(ref, hyp) for ref, hyp in zip(references, hypotheses)]\n",
    "    average_rouge1 = sum(score['rouge1'].fmeasure for score in scores) / len(scores)\n",
    "    average_rougeL = sum(score['rougeL'].fmeasure for score in scores) / len(scores)\n",
    "    return average_rouge1, average_rougeL\n",
    "\n",
    "# Example usage\n",
    "rouge1, rougeL = evaluate_rouge(loaded_model, val_loader, french_tokenizer, bassa_tokenizer, device)\n",
    "print(f\"ROUGE-1 Score: {rouge1:.4f}\")\n",
    "print(f\"ROUGE-L Score: {rougeL:.4f}\")\n",
    "```\n",
    "\n",
    "### Summary\n",
    "\n",
    "- **BLEU**: Commonly used for machine translation, focusing on precision and considering the order of words. Suitable for evaluating how close the candidate translation is to the reference translation.\n",
    "- **ROUGE**: Typically used for summarization but can be used for translation as well, focusing on recall and coverage of the reference translation.\n",
    "\n",
    "Using both BLEU and ROUGE scores can provide a more comprehensive evaluation of your translation model, giving insights into both precision and recall aspects."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-mps",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
