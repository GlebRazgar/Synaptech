{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "import logging\n",
    "import traceback\n",
    "import shutil\n",
    "import zipfile\n",
    "import random\n",
    "import re\n",
    "import numpy as np\n",
    "import contextlib\n",
    "import mne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the logger\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_contents_to_parent_and_delete_sub(subfolder, parentfolder, expected_content_count=None):\n",
    "    \"\"\"\n",
    "    Moves all contents from a subfolder to its parent folder and deletes the subfolder.\n",
    "\n",
    "    Parameters:\n",
    "        subfolder (str): The name (relative path) of the subfolder whose contents are to be moved.\n",
    "        parentfolder (str): The absolute path to the parent folder where contents will be moved.\n",
    "        expected_content_count (int, optional): Expected number of items in the subfolder.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    subfolder_path = os.path.join(parentfolder, subfolder)\n",
    "    if expected_content_count is not None:\n",
    "        actual_content_count = len(os.listdir(subfolder_path))\n",
    "        assert actual_content_count == expected_content_count, (\n",
    "            f\"Expected {expected_content_count} items, but found {actual_content_count} in {subfolder_path}\"\n",
    "        )\n",
    "    # Move contents of subfolder to parentfolder\n",
    "    for item in os.listdir(subfolder_path):\n",
    "        item_path = os.path.join(subfolder_path, item)\n",
    "        shutil.move(item_path, parentfolder)\n",
    "    \n",
    "    # Delete all folders between parentfolder and subfolder\n",
    "    current_path = subfolder_path\n",
    "    while current_path != parentfolder:\n",
    "        parent_path = os.path.dirname(current_path)\n",
    "        os.rmdir(current_path)\n",
    "        current_path = parent_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unzip_and_rename_in_folder(folder, remove=False):\n",
    "    \"\"\"\n",
    "    Unzips all zip files in the specified folder and renames the extracted folders.\n",
    "\n",
    "    Parameters:\n",
    "        folder (str): The path to the folder containing zip files to be unzipped.\n",
    "        remove (bool): If True, the zip files will be deleted after extraction.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    unzipped_marker = os.path.join(folder, '.unzipped')\n",
    "    if os.path.exists(unzipped_marker):\n",
    "        logger.info(f\"Folder {folder} is already unzipped. Exiting early.\")\n",
    "        return\n",
    "\n",
    "    assert all(f.endswith('.zip') or f.startswith('.') for f in os.listdir(folder)), (\n",
    "        f\"Not all files in {folder} are zip files or ignored files. Please delete non-zip files and re-run.\"\n",
    "    )\n",
    "    zip_file_count = sum(1 for f in os.listdir(folder) if f.endswith('.zip'))\n",
    "    logger.info(f\"Unzipping and renaming {zip_file_count} files in folder: {folder}\")\n",
    "    for zip_file in os.listdir(folder):\n",
    "        zip_file_path = os.path.join(folder, zip_file)\n",
    "        if zip_file.endswith('.zip'):\n",
    "            logger.info(f\"Unzipping {zip_file_path}...\")\n",
    "            try:\n",
    "                with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "                    zip_ref.extractall(os.path.dirname(zip_file_path))\n",
    "                if remove:\n",
    "                    os.remove(zip_file_path)  # Remove the zip file after extraction\n",
    "                    logger.info(f\"Unzipped and removed {zip_file_path}\")\n",
    "                else:\n",
    "                    logger.info(f\"Unzipped {zip_file_path}\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error unzipping {zip_file_path}, {e}\")\n",
    "                traceback.print_exc()\n",
    "    for i, unzipped_folder in enumerate(\n",
    "        f for f in os.listdir(folder) if not f.endswith('.zip') and not f.startswith('.')\n",
    "    ):\n",
    "        from_name = os.path.join(folder, unzipped_folder)\n",
    "        to_name = os.path.join(folder, f'folder_{i}')\n",
    "        os.rename(from_name, to_name)\n",
    "\n",
    "    # Create the .unzipped marker file\n",
    "    with open(unzipped_marker, 'w') as marker_file:\n",
    "        marker_file.write('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arrange_folders(dataset_folder):\n",
    "    \"\"\"\n",
    "    Rearranges the files in the dataset folder after downloading and unzipping.\n",
    "\n",
    "    This function organizes the dataset by moving participant folders into the main dataset folder,\n",
    "    ensuring that relevant files are not nested within subdirectories.\n",
    "\n",
    "    Parameters:\n",
    "        dataset_folder (str): The path to the main dataset folder.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    arranged_marker = os.path.join(dataset_folder, '.arranged')\n",
    "    if os.path.exists(arranged_marker):\n",
    "        logger.info(f\"Dataset folder {dataset_folder} is already arranged. Exiting early.\")\n",
    "        return\n",
    "\n",
    "    participant_count = 0\n",
    "    unzipped_folders = [\n",
    "        f for f in os.listdir(dataset_folder) if os.path.isdir(os.path.join(dataset_folder, f))\n",
    "    ]\n",
    "    for folder in unzipped_folders:\n",
    "        # Move participant folders into dataset folder (so they are not nested)\n",
    "        desired_subfolder = os.path.join(folder, 'derivatives', 'meg_derivatives')\n",
    "        move_contents_to_parent_and_delete_sub(desired_subfolder, dataset_folder)\n",
    "    \n",
    "    # Go through participant folders and move nested relevant files up\n",
    "    participant_folders = [\n",
    "        f for f in os.listdir(dataset_folder) if not f.startswith('.') and not f.endswith('.zip')\n",
    "    ]\n",
    "    for participant_folder in participant_folders:\n",
    "        participant_count += 1\n",
    "        desired_subfolder = os.path.join('ses-meg', 'meg')\n",
    "        participant_folder_path = os.path.join(dataset_folder, participant_folder)\n",
    "        move_contents_to_parent_and_delete_sub(desired_subfolder, participant_folder_path)\n",
    "\n",
    "    # Rename fif files to standardized format\n",
    "    for participant_folder in participant_folders: \n",
    "        participant_folder_path = os.path.join(dataset_folder, participant_folder)\n",
    "        for file in os.listdir(participant_folder_path):\n",
    "            match = re.match(r'.*run-(0[1-6]).*', file)\n",
    "            if match:\n",
    "                new_file_name = f'run_{match.group(1)}' + os.path.splitext(file)[1]\n",
    "                os.rename(\n",
    "                    os.path.join(participant_folder_path, file),\n",
    "                    os.path.join(participant_folder_path, new_file_name)\n",
    "                )\n",
    "\n",
    "    # Ensure the dataset folder contains the expected number of participant folders\n",
    "    non_zip_non_dot_folders = [\n",
    "        f for f in os.listdir(dataset_folder) if not f.startswith('.') and not f.endswith('.zip')\n",
    "    ]\n",
    "    assert len(non_zip_non_dot_folders) == participant_count, (\n",
    "        f\"ERROR: Dataset folder contains {len(non_zip_non_dot_folders)} folders, but \"\n",
    "        f\"{participant_count} (participant) folders are expected.\"\n",
    "    )\n",
    "\n",
    "    # Create the .arranged marker file\n",
    "    with open(arranged_marker, 'w') as marker_file:\n",
    "        marker_file.write('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomize_subject_data(dataset_folder, train_percentage=70, val_percentage=20):\n",
    "    \"\"\"\n",
    "    Randomizes and splits participant data into training, validation, and test sets.\n",
    "\n",
    "    Parameters:\n",
    "        dataset_folder (str): The path to the main dataset folder.\n",
    "        train_percentage (int): The percentage of data allocated to the training set.\n",
    "        val_percentage (int): The percentage of data allocated to the validation set.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    participant_folders = [\n",
    "        f for f in os.listdir(dataset_folder) if os.path.isdir(os.path.join(dataset_folder, f))\n",
    "    ]\n",
    "    random.seed(42)\n",
    "    random.shuffle(participant_folders)\n",
    "\n",
    "    total_participants = len(participant_folders)\n",
    "    train_count = int(total_participants * train_percentage / 100)\n",
    "    val_count = int(total_participants * val_percentage / 100)\n",
    "\n",
    "    train_folder = os.path.join(dataset_folder, 'train')\n",
    "    val_folder = os.path.join(dataset_folder, 'val')\n",
    "    test_folder = os.path.join(dataset_folder, 'test')\n",
    "\n",
    "    os.makedirs(train_folder, exist_ok=True)\n",
    "    os.makedirs(val_folder, exist_ok=True)\n",
    "    os.makedirs(test_folder, exist_ok=True)\n",
    "\n",
    "    for i, participant_folder in enumerate(participant_folders):\n",
    "        participant_folder_path = os.path.join(dataset_folder, participant_folder)\n",
    "        if i < train_count:\n",
    "            shutil.move(participant_folder_path, train_folder)\n",
    "        elif i < train_count + val_count:\n",
    "            shutil.move(participant_folder_path, val_folder)\n",
    "        else:\n",
    "            shutil.move(participant_folder_path, test_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_frames(dataset_path, mode='train'):\n",
    "    \"\"\"\n",
    "    Counts the number of frames in the dataset and returns the frame count.\n",
    "\n",
    "    Parameters:\n",
    "        dataset_path (str): The path to the main dataset folder.\n",
    "        mode (str): The dataset mode ('train', 'val', or 'test').\n",
    "\n",
    "    Returns:\n",
    "        tuple: (frame_count, eeg_channel_count, meg_channel_count)\n",
    "    \"\"\"\n",
    "    mode_path = os.path.join(dataset_path, mode)\n",
    "    frame_count_file = os.path.join(mode_path, '.frameCount')\n",
    "\n",
    "    if os.path.exists(frame_count_file):\n",
    "        with open(frame_count_file, 'r') as f:\n",
    "            frame_count = int(f.read().strip())\n",
    "        logger.info(f\"Loaded cached frame count: {frame_count}\")\n",
    "    else:\n",
    "        eeg_frame_count = 0\n",
    "        meg_frame_count = 0\n",
    "        eeg_channels = 0\n",
    "        meg_channels = 0\n",
    "\n",
    "        participant_folders = [\n",
    "            f for f in os.listdir(mode_path)\n",
    "            if not f.startswith('.') and not f.endswith('.zip') and os.path.isdir(os.path.join(mode_path, f))\n",
    "        ]\n",
    "        \n",
    "        logger.info(f\"Counting frames in {mode} dataset...\")\n",
    "        for participant_folder in participant_folders:\n",
    "            fif_files = [\n",
    "                file for file in os.listdir(os.path.join(mode_path, participant_folder))\n",
    "                if file.endswith('.fif')\n",
    "            ]\n",
    "            for fif in fif_files:\n",
    "                fif_path = os.path.join(mode_path, participant_folder, fif)\n",
    "                try:\n",
    "                    with contextlib.redirect_stdout(None), contextlib.redirect_stderr(None):\n",
    "                        raw_data = mne.io.read_raw_fif(fif_path, preload=False, verbose=False)\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error reading {fif_path}, {e}\")\n",
    "                    raise e\n",
    "                eeg_data = raw_data.get_data(picks='eeg')\n",
    "                meg_data = raw_data.get_data(picks='meg')\n",
    "                eeg_frame_count += eeg_data.shape[1]\n",
    "                meg_frame_count += meg_data.shape[1]\n",
    "                eeg_channels = eeg_data.shape[0]\n",
    "                meg_channels = meg_data.shape[0]\n",
    "        assert eeg_frame_count == meg_frame_count, \"EEG and MEG frame count not equal.\"\n",
    "        frame_count = eeg_frame_count\n",
    "        logger.info(f\"Successfully counted {frame_count} frames in {mode} dataset...\")\n",
    "\n",
    "        with open(frame_count_file, 'w') as f:\n",
    "            f.write(str(frame_count))\n",
    "\n",
    "    return frame_count, eeg_channels, meg_channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_chunk_number(frame_count, eeg_channel_count, meg_channel_count,\n",
    "                           memory_limit='32', vram_limit='16',\n",
    "                           memory_chunk_size='4', vram_chunk_size='8',\n",
    "                           eeg_values_type=np.float16, meg_values_type=np.float16):\n",
    "    \"\"\"\n",
    "    Calculates and logs the number of chunks for memory and VRAM usage.\n",
    "\n",
    "    Parameters:\n",
    "        frame_count (int): Total number of frames.\n",
    "        eeg_channel_count (int): Number of EEG channels.\n",
    "        meg_channel_count (int): Number of MEG channels.\n",
    "        memory_limit (str): Memory limit in GB.\n",
    "        vram_limit (str): VRAM limit in GB.\n",
    "        memory_chunk_size (str): Memory chunk size in GB.\n",
    "        vram_chunk_size (str): VRAM chunk size in GB.\n",
    "        eeg_values_type (dtype): Data type for EEG values.\n",
    "        meg_values_type (dtype): Data type for MEG values.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    logger.info(\"Calculating chunk sizes...\")\n",
    "    eeg_value_type_size = np.dtype(eeg_values_type).itemsize\n",
    "    total_eeg_size = eeg_channel_count * frame_count * eeg_value_type_size\n",
    "\n",
    "    meg_value_type_size = np.dtype(meg_values_type).itemsize\n",
    "    total_meg_size = meg_channel_count * frame_count * meg_value_type_size\n",
    "\n",
    "    total_size = total_eeg_size + total_meg_size\n",
    "    \n",
    "    logger.info(f\"Size of EEG and MEG data in dataset: {total_size / (1024**3):.2f} GB\")\n",
    "\n",
    "    memory_chunk_size_bytes = int(memory_chunk_size) * (1024 ** 3)\n",
    "    vram_chunk_size_bytes = int(vram_chunk_size) * (1024 ** 3)\n",
    "\n",
    "    memory_chunk_number = total_size // memory_chunk_size_bytes\n",
    "    vram_chunk_number = total_size // vram_chunk_size_bytes\n",
    "\n",
    "    logger.info(f\"Memory Chunk Number: {memory_chunk_number}\")\n",
    "    logger.info(f\"VRAM Chunk Number: {vram_chunk_number}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = os.path.join('data', 'openfmri')\n",
    "default_download_urls = [  # Links to normalized data of all participants\n",
    "    # \"https://s3.amazonaws.com/openneuro/ds000117/ds000117_R1.0.0/compressed/ds000117_R1.0.0_derivatives_sub01-04.zip\",\n",
    "    # \"https://s3.amazonaws.com/openneuro/ds000117/ds000117_R1.0.0/compressed/ds000117_R1.0.0_derivatives_sub05-08.zip\",\n",
    "    # \"https://s3.amazonaws.com/openneuro/ds000117/ds000117_R1.0.0/compressed/ds000117_R1.0.0_derivatives_sub09-12.zip\",\n",
    "    # \"https://s3.amazonaws.com/openneuro/ds000117/ds000117_R1.0.0/compressed/ds000117_R1.0.0_derivatives_sub13-16.zip\"\n",
    "]\n",
    "\n",
    "def download_dataset(dataset_path, download_urls):\n",
    "    \"\"\"\n",
    "    Downloads the dataset from the specified URLs to the given dataset path.\n",
    "\n",
    "    Parameters:\n",
    "        dataset_path (str): The directory path where the dataset will be downloaded.\n",
    "        download_urls (list): A list of URLs from which to download the dataset.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    if not os.path.exists(dataset_path):\n",
    "        os.makedirs(dataset_path)\n",
    "    \n",
    "    downloaded_marker = os.path.join(dataset_path, '.downloaded')\n",
    "    \n",
    "    if not os.path.exists(downloaded_marker):\n",
    "        logger.info(f\"Downloading {len(download_urls)} files...\")\n",
    "        for url in download_urls:\n",
    "            file_name = os.path.join(dataset_path, url.split('/')[-1])\n",
    "            if not os.path.exists(file_name):\n",
    "                logger.info(f\"Downloading {file_name}...\")\n",
    "                try:\n",
    "                    os.system(f\"wget -O {file_name} {url}\")\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error downloading file {file_name}, {e}\")\n",
    "                    traceback.print_exc()\n",
    "            else:\n",
    "                logger.info(f\"{file_name} already exists, skipping download.\")\n",
    "        \n",
    "        # Check if all files are downloaded\n",
    "        downloaded_files = [f for f in os.listdir(dataset_path) if f.endswith('.zip')]\n",
    "        if len(downloaded_files) == len(download_urls):\n",
    "            with open(downloaded_marker, 'w') as f:\n",
    "                f.write('Download completed successfully.')\n",
    "            logger.info(f\"Successfully downloaded {len(downloaded_files)} files.\")\n",
    "    else:\n",
    "        logger.info(\"Dataset already downloaded. Skipping download.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download_dataset(dataset_path, default_download_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unzip_and_rename_in_folder(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Arrange Folders\n",
    "arrange_folders(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Count Frames in the dataset\n",
    "randomize_subject_data(dataset_path, train_percentage=70, val_percentage=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Randomze and split subject data\n",
    "randomize_subject_data(dataset_path, train_percentage=70, val_percentage=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.  Count Frames in the dataset\n",
    "frame_count, eeg_channel_count, meg_channel_count = count_frames(dataset_path, mode='train')\n",
    "logger.info(f\"Frame Count: {frame_count}\")\n",
    "logger.info(f\"EEG Channel Count: {eeg_channel_count}\")\n",
    "logger.info(f\"MEG Channel Count: {meg_channel_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Chunk Number\n",
    "calculate_chunk_number(\n",
    "    frame_count,\n",
    "    eeg_channel_count,\n",
    "    meg_channel_count,\n",
    "    memory_limit='32',\n",
    "    vram_limit='16',\n",
    "    memory_chunk_size='4',\n",
    "    vram_chunk_size='8',\n",
    "    eeg_values_type=np.float16,\n",
    "    meg_values_type=np.float16\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "synaptech_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
