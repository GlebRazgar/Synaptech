{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "import logging\n",
    "import traceback\n",
    "import shutil\n",
    "import zipfile\n",
    "import random\n",
    "import re\n",
    "import numpy as np\n",
    "import contextlib\n",
    "import mne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = os.path.join('data', 'openfmri')\n",
    "default_download_urls = [  # Links to normalized data of all participants\n",
    "    # \"https://s3.amazonaws.com/openneuro/ds000117/ds000117_R1.0.0/compressed/ds000117_R1.0.0_derivatives_sub01-04.zip\",\n",
    "    # \"https://s3.amazonaws.com/openneuro/ds000117/ds000117_R1.0.0/compressed/ds000117_R1.0.0_derivatives_sub05-08.zip\",\n",
    "    # \"https://s3.amazonaws.com/openneuro/ds000117/ds000117_R1.0.0/compressed/ds000117_R1.0.0_derivatives_sub09-12.zip\",\n",
    "    # \"https://s3.amazonaws.com/openneuro/ds000117/ds000117_R1.0.0/compressed/ds000117_R1.0.0_derivatives_sub13-16.zip\"\n",
    "]\n",
    "\n",
    "def download_dataset(dataset_path, download_urls):\n",
    "    \"\"\"\n",
    "    Downloads the dataset from the specified URLs to the given dataset path.\n",
    "\n",
    "    Parameters:\n",
    "        dataset_path (str): The directory path where the dataset will be downloaded.\n",
    "        download_urls (list): A list of URLs from which to download the dataset.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    if not os.path.exists(dataset_path):\n",
    "        os.makedirs(dataset_path)\n",
    "    \n",
    "    downloaded_marker = os.path.join(dataset_path, '.downloaded')\n",
    "    \n",
    "    if not os.path.exists(downloaded_marker):\n",
    "        logger.info(f\"Downloading {len(download_urls)} files...\")\n",
    "        for url in download_urls:\n",
    "            file_name = os.path.join(dataset_path, url.split('/')[-1])\n",
    "            if not os.path.exists(file_name):\n",
    "                logger.info(f\"Downloading {file_name}...\")\n",
    "                try:\n",
    "                    os.system(f\"wget -O {file_name} {url}\")\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error downloading file {file_name}, {e}\")\n",
    "                    traceback.print_exc()\n",
    "            else:\n",
    "                logger.info(f\"{file_name} already exists, skipping download.\")\n",
    "        \n",
    "        # Check if all files are downloaded\n",
    "        downloaded_files = [f for f in os.listdir(dataset_path) if f.endswith('.zip')]\n",
    "        if len(downloaded_files) == len(download_urls):\n",
    "            with open(downloaded_marker, 'w') as f:\n",
    "                f.write('Download completed successfully.')\n",
    "            logger.info(f\"Successfully downloaded {len(downloaded_files)} files.\")\n",
    "    else:\n",
    "        logger.info(\"Dataset already downloaded. Skipping download.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download_dataset(dataset_path, default_download_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unzipping & Renaming the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def unzip_and_rename_in_folder(folder, remove=False):\n",
    "    \"\"\"\n",
    "    Unzips all zip files in the specified folder and renames the extracted folders.\n",
    "\n",
    "    Parameters:\n",
    "        folder (str): The path to the folder containing zip files to be unzipped.\n",
    "        remove (bool): If True, the zip files will be deleted after extraction.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    unzipped_marker = os.path.join(folder, '.unzipped')\n",
    "    if os.path.exists(unzipped_marker):\n",
    "        logger.info(f\"Folder {folder} is already unzipped. Exiting early.\")\n",
    "        return\n",
    "\n",
    "    assert all(f.endswith('.zip') or f.startswith('.') for f in os.listdir(folder)), (\n",
    "        f\"Not all files in {folder} are zip files or ignored files. Please delete non-zip files and re-run.\"\n",
    "    )\n",
    "    zip_file_count = sum(1 for f in os.listdir(folder) if f.endswith('.zip'))\n",
    "    logger.info(f\"Unzipping and renaming {zip_file_count} files in folder: {folder}\")\n",
    "    \n",
    "    # Add a progress bar for unzipping\n",
    "    with tqdm(total=zip_file_count, desc=\"Unzipping files\", unit=\"file\") as pbar:\n",
    "        for zip_file in os.listdir(folder):\n",
    "            zip_file_path = os.path.join(folder, zip_file)\n",
    "            if zip_file.endswith('.zip'):\n",
    "                logger.info(f\"Unzipping {zip_file_path}...\")\n",
    "                try:\n",
    "                    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "                        zip_ref.extractall(os.path.dirname(zip_file_path))\n",
    "                    if remove:\n",
    "                        os.remove(zip_file_path)  # Remove the zip file after extraction\n",
    "                        logger.info(f\"Unzipped and removed {zip_file_path}\")\n",
    "                    else:\n",
    "                        logger.info(f\"Unzipped {zip_file_path}\")\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error unzipping {zip_file_path}, {e}\")\n",
    "                    traceback.print_exc()\n",
    "                pbar.update(1)  # Update the progress bar\n",
    "\n",
    "    for i, unzipped_folder in enumerate(\n",
    "        f for f in os.listdir(folder) if not f.endswith('.zip') and not f.startswith('.')\n",
    "    ):\n",
    "        from_name = os.path.join(folder, unzipped_folder)\n",
    "        to_name = os.path.join(folder, f'folder_{i}')\n",
    "        os.rename(from_name, to_name)\n",
    "\n",
    "    # Create the .unzipped marker file\n",
    "    with open(unzipped_marker, 'w') as marker_file:\n",
    "        marker_file.write('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Unzipping and renaming 1 files in folder: data/openfmri\n",
      "Unzipping files:   0%|          | 0/1 [00:00<?, ?file/s]INFO:__main__:Unzipping data/openfmri/ds000117_R1.0.0_derivatives_sub05-08.zip...\n",
      "INFO:__main__:Unzipped data/openfmri/ds000117_R1.0.0_derivatives_sub05-08.zip\n",
      "Unzipping files: 100%|██████████| 1/1 [02:00<00:00, 120.68s/file]\n"
     ]
    }
   ],
   "source": [
    "unzip_and_rename_in_folder(dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arranging folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arrange_folders(dataset_folder):\n",
    "    \"\"\"\n",
    "    Rearranges the files in the dataset folder after downloading and unzipping.\n",
    "\n",
    "    This function organizes the dataset by moving participant folders into the main dataset folder,\n",
    "    extracting the necessary files, and cleaning up unnecessary directories.\n",
    "\n",
    "    Parameters:\n",
    "        dataset_folder (str): The path to the main dataset folder.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    arranged_marker = os.path.join(dataset_folder, '.arranged')\n",
    "    if os.path.exists(arranged_marker):\n",
    "        logger.info(f\"Dataset folder {dataset_folder} is already arranged. Exiting early.\")\n",
    "        return\n",
    "\n",
    "    # Step 1: Move participant folders from 'folder_0/derivatives/meg_derivatives' to 'dataset_folder'\n",
    "    source_folder = os.path.join(dataset_folder, 'folder_0', 'derivatives', 'meg_derivatives')\n",
    "    if not os.path.exists(source_folder):\n",
    "        logger.error(f\"Source folder {source_folder} does not exist.\")\n",
    "        return\n",
    "\n",
    "    participant_folders = [\n",
    "        f for f in os.listdir(source_folder)\n",
    "        if os.path.isdir(os.path.join(source_folder, f))\n",
    "        and f.startswith('sub-')  # Assuming participant folders start with 'sub-'\n",
    "    ]\n",
    "\n",
    "    # Move participant folders to dataset_folder\n",
    "    for participant_folder in participant_folders:\n",
    "        source = os.path.join(source_folder, participant_folder)\n",
    "        destination = os.path.join(dataset_folder, participant_folder)\n",
    "        if os.path.exists(destination):\n",
    "            logger.warning(f\"Destination folder {destination} already exists. Skipping.\")\n",
    "        else:\n",
    "            shutil.move(source, dataset_folder)\n",
    "            logger.info(f\"Moved {source} to {dataset_folder}.\")\n",
    "\n",
    "    # Step 2: For each participant, move files from 'ses-meg/meg/' to participant folder and delete subfolders\n",
    "    participant_folders = [\n",
    "        f for f in os.listdir(dataset_folder)\n",
    "        if os.path.isdir(os.path.join(dataset_folder, f))\n",
    "        and f.startswith('sub-')\n",
    "    ]\n",
    "\n",
    "    for participant_folder in participant_folders:\n",
    "        participant_folder_path = os.path.join(dataset_folder, participant_folder)\n",
    "        source_subfolder = os.path.join(participant_folder_path, 'ses-meg', 'meg')\n",
    "        if not os.path.exists(source_subfolder):\n",
    "            logger.warning(f\"Expected directory {source_subfolder} does not exist. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        # Move contents of 'ses-meg/meg' to participant folder\n",
    "        move_contents_to_parent_and_delete_sub(\n",
    "            source_subfolder,\n",
    "            participant_folder_path,\n",
    "            delete_empty_parent_dirs=True  # New parameter to delete empty parent directories\n",
    "        )\n",
    "\n",
    "    # Step 3: Remove unnecessary folders\n",
    "    folder_0_path = os.path.join(dataset_folder, 'folder_0')\n",
    "    if os.path.exists(folder_0_path):\n",
    "        shutil.rmtree(folder_0_path)\n",
    "        logger.info(f\"Removed unnecessary folder {folder_0_path}.\")\n",
    "\n",
    "    # Create the .arranged marker file\n",
    "    with open(arranged_marker, 'w') as marker_file:\n",
    "        marker_file.write('')\n",
    "\n",
    "    logger.info(\"Arrangement of folders completed successfully.\")\n",
    "\n",
    "\n",
    "def move_contents_to_parent_and_delete_sub(source_folder, destination_folder, delete_empty_parent_dirs=False):\n",
    "    \"\"\"\n",
    "    Moves all contents from the source folder to the destination folder and deletes the source folder.\n",
    "    Optionally deletes any empty parent directories of the source folder.\n",
    "\n",
    "    Parameters:\n",
    "        source_folder (str): The absolute path of the source folder whose contents are to be moved.\n",
    "        destination_folder (str): The absolute path to the destination folder where contents will be moved.\n",
    "        delete_empty_parent_dirs (bool): If True, delete empty parent directories after moving contents.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    if not os.path.exists(source_folder):\n",
    "        logger.warning(f\"Source folder {source_folder} does not exist. Skipping.\")\n",
    "        return\n",
    "\n",
    "    # Move contents of source_folder to destination_folder\n",
    "    for item in os.listdir(source_folder):\n",
    "        source_item = os.path.join(source_folder, item)\n",
    "        destination_item = os.path.join(destination_folder, item)\n",
    "        if os.path.exists(destination_item):\n",
    "            logger.warning(f\"Destination item {destination_item} already exists. Skipping.\")\n",
    "            continue\n",
    "        shutil.move(source_item, destination_folder)\n",
    "        logger.info(f\"Moved {source_item} to {destination_folder}.\")\n",
    "\n",
    "    # Delete the source folder\n",
    "    shutil.rmtree(source_folder)\n",
    "    logger.info(f\"Removed folder {source_folder}.\")\n",
    "\n",
    "    # Optionally delete empty parent directories\n",
    "    if delete_empty_parent_dirs:\n",
    "        current_path = os.path.dirname(source_folder)\n",
    "        while current_path != destination_folder:\n",
    "            if not os.listdir(current_path):\n",
    "                os.rmdir(current_path)\n",
    "                logger.info(f\"Removed empty parent directory {current_path}.\")\n",
    "                current_path = os.path.dirname(current_path)\n",
    "            else:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arrange_folders(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_dataset_files(dataset_path):\n",
    "    \"\"\"\n",
    "    Renames all files in the dataset according to specified pattern.\n",
    "    \n",
    "    For each participant folder (sub-XX), it finds files matching the pattern:\n",
    "    'sub-XX_ses-meg_task-facerecognition_run-YY_proc-tsss_meg.fif' or '.txt'\n",
    "    and renames them to 'run_YY.fif' or 'run_YY.txt'.\n",
    "\n",
    "    Parameters:\n",
    "        dataset_path (str): The path to the dataset folder containing participant folders.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Get all participant folders (directories starting with 'sub-')\n",
    "    participant_folders = [\n",
    "        f for f in os.listdir(dataset_path)\n",
    "        if os.path.isdir(os.path.join(dataset_path, f)) and f.startswith('sub-')\n",
    "    ]\n",
    "\n",
    "    for participant_folder in participant_folders:\n",
    "        participant_path = os.path.join(dataset_path, participant_folder)\n",
    "        \n",
    "        # Process each file in the participant folder\n",
    "        for file_name in os.listdir(participant_path):\n",
    "            # Match the pattern for .fif and .txt files\n",
    "            match = re.match(\n",
    "                r'^sub-\\d+_ses-meg_task-facerecognition_run-(\\d{2})_proc-tsss_(meg|log)\\.(fif|txt)$',\n",
    "                file_name\n",
    "            )\n",
    "            \n",
    "            if match:\n",
    "                run_number = match.group(1)\n",
    "                file_type = match.group(2)\n",
    "                extension = match.group(3)\n",
    "                \n",
    "                # Determine new filename based on file type\n",
    "                if file_type == 'meg':\n",
    "                    new_file_name = f'run_{run_number}.fif'\n",
    "                elif file_type == 'log':\n",
    "                    new_file_name = f'run_{run_number}.txt'\n",
    "                else:\n",
    "                    continue\n",
    "                \n",
    "                # Create full paths for source and destination\n",
    "                source = os.path.join(participant_path, file_name)\n",
    "                destination = os.path.join(participant_path, new_file_name)\n",
    "                \n",
    "                # Rename file if destination doesn't exist\n",
    "                if os.path.exists(destination):\n",
    "                    logger.warning(f\"Destination file {destination} already exists. Skipping renaming of {source}.\")\n",
    "                    continue\n",
    "                    \n",
    "                try:\n",
    "                    os.rename(source, destination)\n",
    "                    logger.info(f\"Renamed {source} to {destination}.\")\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Failed to rename {source} to {destination}: {e}\")\n",
    "            else:\n",
    "                logger.debug(f\"File {file_name} does not match the expected pattern. Skipping.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rename_dataset_files(dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomize_subject_data(dataset_folder, train_percentage=70, val_percentage=20):\n",
    "    \"\"\"\n",
    "    Randomizes and splits participant data into training, validation, and test sets.\n",
    "\n",
    "    Parameters:\n",
    "        dataset_folder (str): The path to the main dataset folder.\n",
    "        train_percentage (int): The percentage of data allocated to the training set.\n",
    "        val_percentage (int): The percentage of data allocated to the validation set.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    participant_folders = [\n",
    "        f for f in os.listdir(dataset_folder) if os.path.isdir(os.path.join(dataset_folder, f))\n",
    "    ]\n",
    "    random.seed(42)\n",
    "    random.shuffle(participant_folders)\n",
    "\n",
    "    total_participants = len(participant_folders)\n",
    "    train_count = int(total_participants * train_percentage / 100)\n",
    "    val_count = int(total_participants * val_percentage / 100)\n",
    "\n",
    "    train_folder = os.path.join(dataset_folder, 'train')\n",
    "    val_folder = os.path.join(dataset_folder, 'val')\n",
    "    test_folder = os.path.join(dataset_folder, 'test')\n",
    "\n",
    "    os.makedirs(train_folder, exist_ok=True)\n",
    "    os.makedirs(val_folder, exist_ok=True)\n",
    "    os.makedirs(test_folder, exist_ok=True)\n",
    "\n",
    "    for i, participant_folder in enumerate(participant_folders):\n",
    "        participant_folder_path = os.path.join(dataset_folder, participant_folder)\n",
    "        if i < train_count:\n",
    "            shutil.move(participant_folder_path, train_folder)\n",
    "        elif i < train_count + val_count:\n",
    "            shutil.move(participant_folder_path, val_folder)\n",
    "        else:\n",
    "            shutil.move(participant_folder_path, test_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "randomize_subject_data(dataset_path, train_percentage=70, val_percentage=20)\n",
    "# Might need a manual re-shuffling after, if no subject folders are in the val or train or test dir after you run this function."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "synaptech_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
